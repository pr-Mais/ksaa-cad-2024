@article{Kamalloo2023,
  abstract = {The ever-increasing size of language models curtails their widespread availability to the community, thereby galvanizing many companies into offering access to large language models through APIs. One particular type, suitable for dense retrieval, is a semantic embedding service that builds vector representations of input text. With a growing number of publicly available APIs, our goal in this paper is to analyze existing offerings in realistic retrieval scenarios, to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we investigate the capabilities of existing semantic embedding APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate these services on two standard benchmarks, BEIR and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective in English, in contrast to the standard practice of employing them as first-stage retrievers. For non-English retrieval, re-ranking still improves the results, but a hybrid model with BM25 works best, albeit at a higher cost. We hope our work lays the groundwork for evaluating semantic embedding APIs that are critical in search and more broadly, for information access.},
  author   = {Ehsan Kamalloo and Xinyu Zhang and Odunayo Ogundepo and Nandan Thakur and David Alfonso-Hermelo and Mehdi Rezagholizadeh and Jimmy Lin},
  month    = {5},
  title    = {Evaluating Embedding APIs for Information Retrieval},
  url      = {http://arxiv.org/abs/2305.06300},
  year     = {2023}
}
@article{Simonyan2014,
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  author   = {Karen Simonyan and Andrew Zisserman},
  month    = {9},
  title    = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  url      = {http://arxiv.org/abs/1409.1556},
  year     = {2014}
}
@article{ILSVRC15,
  author  = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  title   = {ImageNet Large Scale Visual Recognition Challenge},
  year    = {2015},
  journal = {International Journal of Computer Vision (IJCV)},
  doi     = {10.1007/s11263-015-0816-y},
  volume  = {115},
  number  = {3},
  pages   = {211-252}
}
@article{Szegedy2014,
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  author   = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
  month    = {9},
  title    = {Going Deeper with Convolutions},
  url      = {http://arxiv.org/abs/1409.4842},
  year     = {2014}
}
@article{He2015,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers 8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57 error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC and COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  author   = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  month    = {12},
  title    = {Deep Residual Learning for Image Recognition},
  url      = {http://arxiv.org/abs/1512.03385},
  year     = {2015}
}
@inproceedings{inoue2021,
  title     = {The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models},
  author    = {Inoue, Go  and
               Alhafni, Bashar  and
               Baimukan, Nurpeiis  and
               Bouamor, Houda  and
               Habash, Nizar},
  booktitle = {Proceedings of the Sixth Arabic Natural Language Processing Workshop},
  month     = apr,
  year      = {2021},
  address   = {Kyiv, Ukraine (Online)},
  publisher = {Association for Computational Linguistics},
  abstract  = {In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.}
}
@inproceedings{Huang2020,
  abstract  = {Search in social networks such as Facebook poses different challenges than in classical web search: besides the query text, it is important to take into account the searcher's context to provide relevant results. Their social graph is an integral part of this context and is a unique aspect of Facebook search. While embedding-based retrieval (EBR) has been applied in web search engines for years, Facebook search was still mainly based on a Boolean matching model. In this paper, we discuss the techniques for applying EBR to a Facebook Search system. We introduce the unified embedding framework developed to model semantic embeddings for personalized search, and the system to serve embedding-based retrieval in a typical search system based on an inverted index. We discuss various tricks and experiences on end-to-end optimization of the whole system, including ANN parameter tuning and full-stack optimization. Finally, we present our progress on two selected advanced topics about modeling. We evaluated EBR on verticals for Facebook Search with significant metrics gains observed in online A/B experiments. We believe this paper will provide useful insights and experiences to help people on developing embedding-based retrieval systems in search engines.},
  author    = {Jui Ting Huang and Ashish Sharma and Shuying Sun and Li Xia and David Zhang and Philip Pronin and Janani Padmanabhan and Giuseppe Ottaviano and Linjun Yang},
  doi       = {10.1145/3394486.3403305},
  isbn      = {9781450379984},
  journal   = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  keywords  = {deep learning,embedding,information retrieval,search},
  month     = {8},
  pages     = {2553-2561},
  publisher = {Association for Computing Machinery},
  title     = {Embedding-based Retrieval in Facebook Search},
  year      = {2020}
}

@article{Antoun2020-araBERT,
  abstract = {The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/arabert hoping to encourage research and applications for Arabic NLP.},
  author   = {Wissam Antoun and Fady Baly and Hazem Hajj},
  month    = {2},
  title    = {AraBERT: Transformer-based Model for Arabic Language Understanding},
  url      = {http://arxiv.org/abs/2003.00104},
  year     = {2020}
}
@article{Antoun2020-araELECTRA,
  abstract = {Advances in English language representation enabled a more sample-efficient pre-training task by Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). Which, instead of training a model to recover masked tokens, it trains a discriminator model to distinguish true input tokens from corrupted tokens that were replaced by a generator network. On the other hand, current Arabic language representation approaches rely only on pretraining via masked language modeling. In this paper, we develop an Arabic language representation model, which we name AraELECTRA. Our model is pretrained using the replaced token detection objective on large Arabic text corpora. We evaluate our model on multiple Arabic NLP tasks, including reading comprehension, sentiment analysis, and named-entity recognition and we show that AraELECTRA outperforms current state-of-the-art Arabic language representation models, given the same pretraining data and with even a smaller model size.},
  author   = {Wissam Antoun and Fady Baly and Hazem Hajj},
  month    = {12},
  title    = {AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding},
  url      = {http://arxiv.org/abs/2012.15516},
  year     = {2020}
}
@inproceedings{Linting2021,
  abstract  = {The recent ``Text-to-Text Transfer Transformer'' (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent ``accidental translation'' in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.},
  author    = {Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},
  city      = {Online},
  doi       = {10.18653/v1/2021.naacl-main.41},
  editor    = {Kristina Toutanova and Anna Rumshisky and Luke Zettlemoyer and Dilek Hakkani-Tur and Iz Beltagy and Steven Bethard and Ryan Cotterell and Tanmoy Chakraborty and Yichao Zhou},
  journal   = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {6},
  pages     = {483-498},
  publisher = {Association for Computational Linguistics},
  title     = {mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer},
  url       = {https://aclanthology.org/2021.naacl-main.41},
  year      = {2021}
}
@article{Nagoudi2021,
  abstract = {Transfer learning with a unified Transformer framework (T5) that converts all language problems into a text-to-text format was recently proposed as a simple and effective transfer learning approach. Although a multilingual version of the T5 model (mT5) was also introduced, it is not clear how well it can fare on non-English tasks involving diverse data. To investigate this question, we apply mT5 on a language with a wide variety of dialects--Arabic. For evaluation, we introduce a novel benchmark for ARabic language GENeration (ARGEN), covering seven important tasks. For model comparison, we pre-train three powerful Arabic T5-style models and evaluate them on ARGEN. Although pre-trained with ~49 less data, our new models perform significantly better than mT5 on all ARGEN tasks (in 52 out of 59 test sets) and set several new SOTAs. Our models also establish new SOTA on the recently-proposed, large Arabic language understanding evaluation benchmark ARLUE (Abdul-Mageed et al., 2021). Our new models are publicly available. We also link to ARGEN datasets through our repository: https://github.com/UBC-NLP/araT5.},
  author   = {El Moatez Billah Nagoudi and AbdelRahim Elmadany and Muhammad Abdul-Mageed},
  month    = {8},
  title    = {AraT5: Text-to-Text Transformers for Arabic Language Generation},
  url      = {http://arxiv.org/abs/2109.12068},
  year     = {2021}
}
@article{Kudo2018,
  abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
  author   = {Taku Kudo and John Richardson},
  month    = {8},
  title    = {SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  url      = {http://arxiv.org/abs/1808.06226},
  year     = {2018}
}
@article{Raffel2019,
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  author   = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  month    = {10},
  title    = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  url      = {http://arxiv.org/abs/1910.10683},
  year     = {2019}
}
@misc{killian1995,
  author      = {Anne Stallman Kilian and William E. Nagy and P. David Pearson and Richard C. Anderson and Georgia Earnest Garcia},
  institution = {University of Illinois at Urbana-Champaign},
  title       = {Learning vocabulary from context: effects of focusing attention on individual words during reading},
  url         = {https://core.ac.uk/display/4826065},
  year        = {1995}
}
@article{devlin2018,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a Rad-ford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result , the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7 (4.6 absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  author   = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova Google and A I Language},
  isbn     = {1810.04805v2},
  title    = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url      = {https://github.com/tensorflow/tensor2tensor},
  year     = {2018}
}
@article{Torfi2020,
  abstract = {Natural Language Processing (NLP) helps empower intelligent machines by enhancing a better understanding of the human language for linguistic-based human-computer communication. Recent developments in computational power and the advent of large amounts of linguistic data have heightened the need and demand for automating semantic analysis using data-driven approaches. The utilization of data-driven strategies is pervasive now due to the significant improvements demonstrated through the usage of deep learning methods in areas such as Computer Vision, Automatic Speech Recognition, and in particular, NLP. This survey categorizes and addresses the different aspects and applications of NLP that have benefited from deep learning. It covers core NLP tasks and applications and describes how deep learning methods and models advance these areas. We further analyze and compare different approaches and state-of-the-art models.},
  author   = {Amirsina Torfi and Rouzbeh A. Shirvani and Yaser Keneshloo and Nader Tavaf and Edward A. Fox},
  month    = {3},
  title    = {Natural Language Processing Advancements By Deep Learning: A Survey},
  url      = {http://arxiv.org/abs/2003.01200},
  year     = {2020}
}
@article{Khurana2023,
  abstract  = {Natural language processing (NLP) has recently gained much attention for representing and analyzing human language computationally. It has spread its applications in various fields such as machine translation, email spam detection, information extraction, summarization, medical, and question answering etc. In this paper, we first distinguish four phases by discussing different levels of NLP and components of Natural Language Generation followed by presenting the history and evolution of NLP. We then discuss in detail the state of the art presenting the various applications of NLP, current trends, and challenges. Finally, we present a discussion on some available datasets, models, and evaluation metrics in NLP.},
  author    = {Diksha Khurana and Aditya Koli and Kiran Khatter and Sukhdev Singh},
  doi       = {10.1007/s11042-022-13428-4},
  issn      = {15737721},
  issue     = {3},
  journal   = {Multimedia Tools and Applications},
  keywords  = {NLP applications,NLP evaluation metrics,Natural language generation,Natural language processing,Natural language understanding},
  month     = {1},
  pages     = {3713-3744},
  publisher = {Springer},
  title     = {Natural language processing: state of the art, current trends and challenges},
  volume    = {82},
  year      = {2023}
}
@article{Ghaddar2024,
  abstract = {Pretraining monolingual language models have been proven to be vital for performance in Arabic Natural Language Processing (NLP) tasks. In this paper, we conduct a comprehensive study on the role of data in Arabic Pretrained Language Models (PLMs). More precisely, we reassess the performance of a suite of state-of-the-art Arabic PLMs by retraining them on massive-scale, high-quality Arabic corpora. We have significantly improved the performance of the leading Arabic encoder-only BERT-base and encoder-decoder T5-base models on the ALUE and ORCA leaderboards, thereby reporting state-of-the-art results in their respective model categories. In addition, our analysis strongly suggests that pretraining data by far is the primary contributor to performance, surpassing other factors. Our models and source code are publicly available at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch.},
  author   = {Abbas Ghaddar and Philippe Langlais and Mehdi Rezagholizadeh and Boxing Chen},
  month    = {1},
  title    = {On the importance of Data Scale in Pretraining Arabic Language Models},
  url      = {http://arxiv.org/abs/2401.07760},
  year     = {2024}
}
@inproceedings{Alghamdi2023,
  abstract  = {Developing monolingual large Pre-trained Language Models (PLMs) is shown to be very successful in handling different tasks in Natural Language Processing (NLP). In this work, we present AraMUS, the largest Arabic PLM with 11B parameters trained on 529GB of high-quality Arabic textual data. AraMUS achieves state-of-the-art performances on a diverse set of Arabic classification and generative tasks. Moreover, AraMUS shows impressive few-shot learning abilities compared with the best existing Arabic PLMs.},
  author    = {Asaad Alghamdi and Xinyu Duan and Wei Jiang and Zhenhai Wang and Yimeng Wu and Qingrong Xia and Zhefeng Wang and Yi Zheng and Mehdi Rezagholizadeh and Baoxing Huai and Peilun Cheng and Abbas Ghaddar},
  city      = {Toronto, Canada},
  doi       = {10.18653/v1/2023.findings-acl.181},
  editor    = {Anna Rogers and Jordan Boyd-Graber and Naoaki Okazaki},
  journal   = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = {7},
  pages     = {2883-2894},
  publisher = {Association for Computational Linguistics},
  title     = {AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing},
  url       = {https://aclanthology.org/2023.findings-acl.181},
  year      = {2023}
}
@article{Griffiths2004,
  abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. &amp; Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying “hot topics” by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
  author   = {Thomas L Griffiths and Mark Steyvers},
  doi      = {10.1073/pnas.0307752101},
  journal  = {Proceedings of the National Academy of Sciences},
  pages    = {5228-5235},
  title    = {Finding scientific topics},
  volume   = {101},
  url      = {https://www.pnas.org/doi/abs/10.1073/pnas.0307752101},
  year     = {2004}
}
@inproceedings{Tran2022,
  abstract  = {The reverse dictionary task is a sequence-to-vector task in which a gloss is provided as input, and the output must be a semantically matching word vector. The reverse dictionary is useful in practical applications such as solving the tip-of-the-tongue problem, helping new language learners, etc. In this paper, we evaluate the effect of a Transformer-based model with cross-lingual zero-shot learning to improve the reverse dictionary performance. Our experiments are conducted in five languages in the CODWOE dataset, including English, French, Italian, Spanish, and Russian. Even if we did not achieve a good ranking in the CODWOE competition, we show that our work partially improves the current baseline from the organizers with a hypothesis on the impact of LSTM in monolingual, multilingual, and zero-shot learning. All the codes are available at https://github.com/honghanhh/codwoe2021.},
  author    = {Thi Hong Hanh Tran and Matej Martinc and Matthew Purver and Senja Pollak},
  city      = {Seattle, United States},
  doi       = {10.18653/v1/2022.semeval-1.12},
  editor    = {Guy Emerson and Natalie Schluter and Gabriel Stanovsky and Ritesh Kumar and Alexis Palmer and Nathan Schneider and Siddharth Singh and Shyam Ratan},
  journal   = {Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)},
  month     = {7},
  pages     = {101-106},
  publisher = {Association for Computational Linguistics},
  title     = {JSI at SemEval-2022 Task 1: CODWOE - Reverse Dictionary: Monolingual and cross-lingual approaches},
  url       = {https://aclanthology.org/2022.semeval-1.12},
  year      = {2022}
}
@inproceedings{Lam2013,
  author    = {Khang Nhut Lam and Jugal Kalita},
  city      = {Atlanta, Georgia},
  editor    = {Lucy Vanderwende and Hal Daumé III and Katrin Kirchhoff},
  journal   = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {6},
  pages     = {524-528},
  publisher = {Association for Computational Linguistics},
  title     = {Creating Reverse Bilingual Dictionaries},
  url       = {https://aclanthology.org/N13-1057},
  year      = {2013}
}
@inproceedings{Chen2022,
  abstract  = {We build a dual-way neural dictionary to retrieve words given definitions, and produce definitions for queried words. The model learns the two tasks simultaneously and handles unknown words via embeddings. It casts a word or a definition to the same representation space through a shared layer, then generates the other form in a multi-task fashion. Our method achieves promising automatic scores on previous benchmarks without extra resources. Human annotators prefer the model's outputs in both reference-less and reference-based evaluation, indicating its practicality. Analysis suggests that multiple objectives benefit learning.},
  author    = {Pinzhen Chen and Zheng Zhao},
  city      = {Online only},
  editor    = {Yulan He and Heng Ji and Sujian Li and Yang Liu and Chua-Hui Chang},
  journal   = {Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  month     = {11},
  pages     = {8-13},
  publisher = {Association for Computational Linguistics},
  title     = {A Unified Model for Reverse Dictionary and Definition Modelling},
  url       = {https://aclanthology.org/2022.aacl-short.2},
  year      = {2022}
}
@inproceedings{Yan2020,
  abstract  = {Reverse dictionary is the task to find the proper target word given the word description. In this paper, we tried to incorporate BERT into this task. However, since BERT is based on the byte-pair-encoding (BPE) subword encoding, it is nontrivial to make BERT generate a word given the description. We propose a simple but effective method to make BERT generate the target word for this specific task. Besides, the cross-lingual reverse dictionary is the task to find the proper target word described in another language. Previous models have to keep two different word embeddings and learn to align these embeddings. Nevertheless, by using the Multilingual BERT (mBERT), we can efficiently conduct the cross-lingual reverse dictionary with one subword embedding, and the alignment between languages is not necessary. More importantly, mBERT can achieve remarkable cross-lingual reverse dictionary performance even without the parallel corpus, which means it can conduct the cross-lingual reverse dictionary with only corresponding monolingual data. Code is publicly available at https://github.com/yhcc/BertForRD.git.},
  author    = {Hang Yan and Xiaonan Li and Xipeng Qiu and Bocao Deng},
  city      = {Online},
  doi       = {10.18653/v1/2020.findings-emnlp.388},
  editor    = {Trevor Cohn and Yulan He and Yang Liu},
  journal   = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  month     = {11},
  pages     = {4329-4338},
  publisher = {Association for Computational Linguistics},
  title     = {BERT for Monolingual and Cross-Lingual Reverse Dictionary},
  url       = {https://aclanthology.org/2020.findings-emnlp.388},
  year      = {2020}
}
@inproceedings{Pilehvar2019,
  abstract  = {Meaning conflation deficiency is one of the main limiting factors of word representations which, given their widespread use at the core of many NLP systems, can lead to inaccurate semantic understanding of the input text and inevitably hamper the performance. Sense representations target this problem. However, their potential impact has rarely been investigated in downstream NLP applications. Through a set of experiments on a state-of-the-art reverse dictionary system based on neural networks, we show that a simple adjustment aimed at addressing the meaning conflation deficiency can lead to substantial improvements.},
  author    = {Mohammad Taher Pilehvar},
  city      = {Minneapolis, Minnesota},
  doi       = {10.18653/v1/N19-1222},
  editor    = {Jill Burstein and Christy Doran and Thamar Solorio},
  journal   = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = {6},
  pages     = {2151-2156},
  publisher = {Association for Computational Linguistics},
  title     = {On the Importance of Distinguishing Word Meaning Representations: A Case Study on Reverse Dictionary Mapping},
  url       = {https://aclanthology.org/N19-1222},
  year      = {2019}
}
@inproceedings{Qi2020,
  abstract  = {A reverse dictionary takes descriptions of words as input and outputs words semantically matching the input descriptions. Reverse dictionaries have great practical value such as solving the tip-of-the-tongue problem and helping new language learners. There have been some online reverse dictionary systems, but they support English reverse dictionary queries only and their performance is far from perfect. In this paper, we present a new open-source online reverse dictionary system named WantWords (https://wantwords.thunlp.org/). It not only significantly outperforms other reverse dictionary systems on English reverse dictionary performance, but also supports Chinese and English-Chinese as well as Chinese-English cross-lingual reverse dictionary queries for the first time. Moreover, it has user-friendly front-end design which can help users find the words they need quickly and easily. All the code and data are available at https://github.com/thunlp/WantWords.},
  author    = {Fanchao Qi and Lei Zhang and Yanhui Yang and Zhiyuan Liu and Maosong Sun},
  city      = {Online},
  doi       = {10.18653/v1/2020.emnlp-demos.23},
  editor    = {Qun Liu and David Schlangen},
  journal   = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = {10},
  pages     = {175-181},
  publisher = {Association for Computational Linguistics},
  title     = {WantWords: An Open-source Online Reverse Dictionary System},
  url       = {https://aclanthology.org/2020.emnlp-demos.23},
  year      = {2020}
}
@inproceedings{Albakry2023,
  abstract  = {A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the ``Tip-of-the-Tongue'' (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each model within the ensemble. In contrast, the most effective solution for the second subtask involves translating the English test definitions into Arabic and applying them to the finetuned models originally trained for the first subtask. This straightforward method achieves the highest score across both subtasks.},
  author    = {Ahmed Elbakry and Mohamed Gabr and Muhammad ElNokrashy and Badr AlKhamissi},
  city      = {Singapore (Hybrid)},
  doi       = {10.18653/v1/2023.arabicnlp-1.43},
  editor    = {Hassan Sawaf and Samhaa El-Beltagy and Wajdi Zaghouani and Walid Magdy and Ahmed Abdelali and Nadi Tomeh and Ibrahim Abu Farha and Nizar Habash and Salam Khalifa and Amr Keleg and Hatem Haddad and Imed Zitouni and Khalil Mrini and Rawan Almatham},
  journal   = {Proceedings of ArabicNLP 2023},
  month     = {12},
  pages     = {477-482},
  publisher = {Association for Computational Linguistics},
  title     = {Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word–Definition Alignment},
  url       = {https://aclanthology.org/2023.arabicnlp-1.43},
  year      = {2023}
}
@inproceedings{Sibaee2023,
  abstract  = {A reverse dictionary takes a descriptive phrase of a particular concept and returns words with definitions that align with that phrase. While many reverse dictionaries cater to languages such as English and are readily available online or have been developed by researchers, there is a notable lack of similar resources for the Arabic language. This paper describes our participation in the Arabic Reverse Dictionary shared task. Our proposed method consists of two main steps: First, we convert word definitions into multidimensional vectors. Then, we train these encoded vectors using the Semi-Decoder model for our target task. Our system secured 2nd place based on the Rank metric for both embeddings (Electra and Sgns).},
  author    = {Serry Sibaee and Samar Ahmad and Ibrahim Khurfan and Vian Sabeeh and Ahmed Bahaaulddin and Hanan Belhaj and Abdullah Alharbi},
  city      = {Singapore (Hybrid)},
  doi       = {10.18653/v1/2023.arabicnlp-1.41},
  editor    = {Hassan Sawaf and Samhaa El-Beltagy and Wajdi Zaghouani and Walid Magdy and Ahmed Abdelali and Nadi Tomeh and Ibrahim Abu Farha and Nizar Habash and Salam Khalifa and Amr Keleg and Hatem Haddad and Imed Zitouni and Khalil Mrini and Rawan Almatham},
  journal   = {Proceedings of ArabicNLP 2023},
  month     = {12},
  pages     = {467-471},
  publisher = {Association for Computational Linguistics},
  title     = {Qamosy at Arabic Reverse Dictionary shared task: Semi Decoder Architecture for Reverse Dictionary with SBERT Encoder},
  url       = {https://aclanthology.org/2023.arabicnlp-1.41},
  year      = {2023}
}
@inproceedings{Qaddoumi2023,
  abstract  = {This paper presents a novel approach to the Arabic Reverse Dictionary Shared Task at WANLP 2023 by leveraging the BERT Multilingual model and introducing modifications augmentation and using a multi attention head. The proposed method aims to enhance the performance of the model in understanding and generating word embeddings for Arabic definitions, both in monolingual and cross-lingual contexts. It achieved good results compared to benchmark and other models in the shared task 1 and 2.},
  author    = {Abdelrahim Qaddoumi},
  city      = {Singapore (Hybrid)},
  doi       = {10.18653/v1/2023.arabicnlp-1.42},
  editor    = {Hassan Sawaf and Samhaa El-Beltagy and Wajdi Zaghouani and Walid Magdy and Ahmed Abdelali and Nadi Tomeh and Ibrahim Abu Farha and Nizar Habash and Salam Khalifa and Amr Keleg and Hatem Haddad and Imed Zitouni and Khalil Mrini and Rawan Almatham},
  journal   = {Proceedings of ArabicNLP 2023},
  month     = {12},
  pages     = {472-476},
  publisher = {Association for Computational Linguistics},
  title     = {Abed at KSAA-RD Shared Task: Enhancing Arabic Word Embedding with Modified BERT Multilingual},
  url       = {https://aclanthology.org/2023.arabicnlp-1.42},
  year      = {2023}
}
@inproceedings{Almatham2023,
  abstract  = {This paper outlines the KSAA-RD shared task, which aims to develop a Reverse Dictionary (RD) system for the Arabic language. RDs allow users to find words based on their meanings or definition. This shared task, KSAA-RD, includes two subtasks: Arabic RD and cross-lingual reverse dictionaries (CLRD). Given a definition (referred to as a ``gloss'') in either Arabic or English, the teams compete to find the most similar word embeddings of their corresponding word. The winning team achieved 24.20 and 12.70 for RD and CLRD, respectively in terms of rank metric. In this paper, we describe the methods employed by the participating teams and offer an outlook for KSAA-RD.},
  author    = {Rawan Al-Matham and Waad Alshammari and Abdulrahman AlOsaimy and Sarah Alhumoud and Asma Wazrah and Afrah Altamimi and Halah Alharbi and Abdullah Alaifi},
  city      = {Singapore (Hybrid)},
  doi       = {10.18653/v1/2023.arabicnlp-1.39},
  editor    = {Hassan Sawaf and Samhaa El-Beltagy and Wajdi Zaghouani and Walid Magdy and Ahmed Abdelali and Nadi Tomeh and Ibrahim Abu Farha and Nizar Habash and Salam Khalifa and Amr Keleg and Hatem Haddad and Imed Zitouni and Khalil Mrini and Rawan Almatham},
  journal   = {Proceedings of ArabicNLP 2023},
  month     = {12},
  pages     = {450-460},
  publisher = {Association for Computational Linguistics},
  title     = {KSAA-RD Shared Task: Arabic Reverse Dictionary},
  url       = {https://aclanthology.org/2023.arabicnlp-1.39},
  year      = {2023}
}
@misc{Nandwani2021,
  abstract  = {Social networking platforms have become an essential means for communicating feelings to the entire world due to rapid expansion in the Internet era. Several people use textual content, pictures, audio, and video to express their feelings or viewpoints. Text communication via Web-based networking media, on the other hand, is somewhat overwhelming. Every second, a massive amount of unstructured data is generated on the Internet due to social media platforms. The data must be processed as rapidly as generated to comprehend human psychology, and it can be accomplished using sentiment analysis, which recognizes polarity in texts. It assesses whether the author has a negative, positive, or neutral attitude toward an item, administration, individual, or location. In some applications, sentiment analysis is insufficient and hence requires emotion detection, which determines an individual’s emotional/mental state precisely. This review paper provides understanding into levels of sentiment analysis, various emotion models, and the process of sentiment analysis and emotion detection from text. Finally, this paper discusses the challenges faced during sentiment and emotion analysis.},
  author    = {Pansy Nandwani and Rupali Verma},
  doi       = {10.1007/s13278-021-00776-6},
  issn      = {18695469},
  issue     = {1},
  journal   = {Social Network Analysis and Mining},
  keywords  = {Affective computing,Natural language processing,Opinion mining,Pre-processing,Word embedding},
  month     = {12},
  publisher = {Springer},
  title     = {A review on sentiment analysis and emotion detection from text},
  volume    = {11},
  url       = {https://link.springer.com/article/10.1007/s13278-021-00776-6},
  year      = {2021}
}
@article{Hedderich2019,
  abstract  = {Popular word embedding methods such as word2vec and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically cannot serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well.},
  author    = {Michael A. Hedderich and Andrew Yates and Dietrich Klakow and Gerard de Melo},
  doi       = {10.18653/V1/W19-0421},
  isbn      = {9781950737192},
  journal   = {IWCS 2019 - Proceedings of the 13th International Conference on Computational Semantics - Long Papers},
  pages     = {247-258},
  publisher = {Association for Computational Linguistics (ACL)},
  title     = {Using multi-sense vector embeddings for reverse dictionaries},
  year      = {2019}
}
@inproceedings{Zhang2019,
  abstract = {A reverse dictionary takes the description of a target word as input and outputs the target word together with other words that match the description. Existing reverse dictionary methods cannot deal with highly variable input queries and low-frequency target words successfully. Inspired by the description-to-word inference process of humans, we propose the multi-channel reverse dictionary model, which can mitigate the two problems simultaneously. Our model comprises a sentence encoder and multiple predictors. The predictors are expected to identify different characteristics of the target word from the input query. We evaluate our model on English and Chinese datasets including both dictionary definitions and human-written descriptions. Experimental results show that our model achieves the state-of-the-art performance, and even outperforms the most popular commercial reverse dictionary system on the human-written description dataset. We also conduct quantitative analyses and a case study to demonstrate the effectiveness and robustness of our model. All the code and data of this work can be obtained on https://github.com/thunlp/MultiRD.},
  author   = {Lei Zhang and Fanchao Qi and Zhiyuan Liu and Yasheng Wang and Qun Liu and Maosong Sun},
  journal  = {Proceedings of the AAAI Conference on Artificial Intelligence},
  month    = {12},
  title    = {Multi-channel Reverse Dictionary Model},
  url      = {http://arxiv.org/abs/1912.08441},
  year     = {2019}
}
@article{Hill2016,
  abstract  = {Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions. We present two applications of these architectures: reverse dictionaries that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing models that understand phrases and sentences.},
  author    = {Felix Hill and Kyunghyun Cho and Anna Korhonen and Yoshua Bengio},
  doi       = {10.1162/TACL_A_00080},
  journal   = {Transactions of the Association for Computational Linguistics},
  month     = {12},
  pages     = {17-30},
  publisher = {MIT Press - Journals},
  title     = {Learning to Understand Phrases by Embedding the Dictionary},
  volume    = {4},
  url       = {https://aclanthology.org/Q16-1002},
  year      = {2016}
}
@article{Brown1966,
  abstract = {The "tip of the tongue" (TOT) phenomenon is a state in which one cannot quite recall a familiar word but can recall words of similar form and meaning. Several hundred such states were precipitated by reading to Ss the difinitions of English words of low frequency and asking them to try to recall the words. It was demonstrated that while in the TOT state, and before recall occurred, Ss had knowledge of some of the letters in the missing word, the number of syllables in it, and the location of the primary stress. The nearer S was to successful recall the more accurate the information he possessed. The recall of parts of words and attributes of words is termed "generic recall." The interpretation offered for generic recall involves the assumption that users of a language possess the mental equivalent of a dictionary. The featurse that figure in generic recall may be entered in the dictionary sooner than other features and so, perhaps, are wired into a more elaborate associative network. These more easily retrieved features of low-frequency words may be the features to which we chiefly attend in word-perception. The features favored by attention, especially the beginnings and endings of words, appear to carry more information than the features that are not favored, in particular the middles of words. © 1966 Academic Press Inc.},
  author   = {Roger Brown and David McNeill},
  doi      = {10.1016/S0022-5371(66)80040-3},
  issn     = {00225371},
  issue    = {4},
  journal  = {Journal of Verbal Learning and Verbal Behavior},
  pages    = {325-337},
  title    = {The "tip of the tongue" phenomenon},
  volume   = {5},
  year     = {1966}
}
@article{Siddique2022,
  abstract  = {Owing to the large information content of adjectives in natural language-based data, we have dealt with the subject of adjective phrases in the paradigm of PNL. We have proposed a GCR form for the same and demonstrated its application in building a specific instance of Reverse Dictionary. A Reverse Dictionary takes a natural language description of a concept in the user's mind as input and generates semantically appropriate word/s as output. For example, it could accept descriptions like 'to run slowly,' 'to run but not very fast,' etc., for the word 'jog'. As Reverse Dictionary deals with natural language, handling perception-based information is important in its context for capturing the user intent. The D-RD, Reverse Dictionary for Descriptive words, designed in this paper, takes a natural language description of a human as input and generates appropriate adjectives for usage in text/speech as output. As a part of the design, we have proposed a novel CWW based similarity measure for calculating semantic similarity between adjective phrases. The D-RD is evaluated against two benchmarks, Onelook.com and WantWords Online RD, and is reported to outperform both for the test set under consideration.},
  author    = {Bushra Siddique and M. M.Sufyan Beg},
  doi       = {10.1109/ACCESS.2022.3158011},
  issn      = {21693536},
  journal   = {IEEE Access},
  keywords  = {CWW,Computing with words,GCR,PNL,RD,generalized constraint representation,precisiated natural language,reverse dictionary},
  pages     = {28385-28396},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Adjective Phrases in PNL and its Application to Reverse Dictionary},
  volume    = {10},
  year      = {2022}
}
@inproceedings{Ardoiz2022,
  abstract  = {This paper presents a novel and linguistic-driven system for the Spanish Reverse Dictionary task of SemEval-2022 Task 1. The aim of this task is the automatic generation of a word using its gloss. The conclusion is that this task results could improve if the quality of the dataset did as well by incorporating high-quality lexicographic data. Therefore, in this paper we analyze the main gaps in the proposed dataset and describe how these limitations could be tackled.},
  author    = {Alfonso Ardoiz and Miguel Ortega-Martin and Óscar Garcia-Sierra and Jorge Álvarez and Ignacio Arranz and Adrián Alonso},
  city      = {Seattle, United States},
  doi       = {10.18653/v1/2022.semeval-1.7},
  editor    = {Guy Emerson and Natalie Schluter and Gabriel Stanovsky and Ritesh Kumar and Alexis Palmer and Nathan Schneider and Siddharth Singh and Shyam Ratan},
  journal   = {Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)},
  month     = {7},
  pages     = {68-74},
  publisher = {Association for Computational Linguistics},
  title     = {MMG at SemEval-2022 Task 1: A Reverse Dictionary approach based on a review of the dataset from a lexicographic perspective},
  url       = {https://aclanthology.org/2022.semeval-1.7},
  year      = {2022}
}
@inproceedings{Mane2022,
  author  = {Sunil B Mane and Harshal Navneet Patil and Kanhaiya Balaji Madaswar and Pranav Nitin Sadavarte},
  doi     = {10.1109/CONIT55038.2022.9848383},
  journal = {2022 2nd International Conference on Intelligent Technologies (CONIT)},
  pages   = {1-5},
  title   = {WordAlchemy: A Transformer-based Reverse Dictionary},
  url     = {https://ieeexplore.ieee.org/abstract/document/9848383},
  year    = {2022}
}
@inproceedings{Siddique2019,
  abstract  = {While dictionaries suit to the needs of readers in finding the meanings of words, it falls short in addressing the needs of the language producers (writers/speakers) for getting an appropriate word corresponding to a concept in mind. ‘Reverse dictionary’ aim to address this problem. It takes a user description (in natural language) of the concept as input and provides a set of words satisfying that description as the output. The problem, although not novel, is of utmost concern in view of the compromises that generation of language producers have to make, the most common of all being the circumlocution. Ranging from books in printed form as earliest attempts to solve this problem, it is found to be addressed comprehensively in the literature only in the near past using diverse approaches based on Information Retrieval System, Mental Dictionary, Semantic Analysis and Neural Language Models. In order to carry out further research on this subject, a critical insight into the existing related works is vital which is provided in this paper. More importantly, identification of the research gaps followed by a discussion of possible enhancements of existing works and related lines of research is presented.},
  author    = {Bushra Siddique and Mirza Mohd Sufyan Beg},
  doi       = {10.1007/978-981-15-1718-1_11},
  isbn      = {9789811517174},
  issn      = {18650937},
  journal   = {Communications in Computer and Information Science},
  keywords  = {Conceptual search,Dictionaries,RD,Reverse dictionary,Thesauruses},
  pages     = {128-139},
  publisher = {Springer},
  title     = {A Review of Reverse Dictionary: Finding Words from Concept Description},
  volume    = {922 CCIS},
  year      = {2019}
}
@inproceedings{Jarrar2019,
  abstract  = {We present a lexicographic search engine built on top of the largest Arabic multilingual database, allowing people to search and retrieve translations, synonyms, definitions, and more. The database currently contains about 150 Arabic multilingual lexicons that we have been digitizing, restructuring, and normalizing over 9 years. It comprises most types of lexical resources, such as modern and classical lexicons, thesauri, glossaries, lexicographic datasets, and (bi/)tri-lingual dictionaries. This is in addition to the Arabic Ontology – an Arabic WordNet with ontologically cleaned content, which is being used to reference and interlink lexical concepts. The search engine was developed with the state-of-the-art design features and according to the W3C’s recommendation and best practices for publishing data on the web, as well as the W3C’s Lemon RDF model. The search engine is publicly available at (https://ontology.birzeit.edu).},
  author    = {Mustafa Jarrar and Hamzeh Amayreh},
  doi       = {10.1007/978-3-030-23281-8_19},
  isbn      = {9783030232801},
  issn      = {16113349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Arabic,Language resources,Lexical semantics,Lexicographic search,Multilingual lexicons,NLP,Online dictionary,RDF,W3C lemon},
  pages     = {234-246},
  publisher = {Springer Verlag},
  title     = {An Arabic-Multilingual Database with a Lexicographic Search Engine},
  volume    = {11608 LNCS},
  year      = {2019}
}
@article{Siddique2023,
  abstract = {In view of the limitation of the forward dictionaries to attend to the needs specific to the language producers, an alternate resource in the form of ‘Reverse Dictionary’ needs to be built. Reverse Dictionary aims to lexicalize the concept in the user’s mind by taking as input a natural language description of the concept and returning word/s that are in semantic correspondence to the description. A critical survey of the existing Reverse Dictionary works is presented in this paper. It is concluded that this problem has been addressed through five categories of approaches, namely, Information Retrieval-based approach, Graph-based approach, Mental dictionary-based approach, Vector Space Model-based approach, and Neural Language Model-based approach. We identify and highlight that the works reported so far do not account for human perceptions in the user input. However, as a NL is a system of perceptions and a Reverse Dictionary deals with natural language input, dealing with perception based information in the user input is important to capture his/her intent. To address the identified research gap, we have considered the concept of Precisiated Natural Language (PNL) based on Zadeh’s paradigm of Computational Theory of Perceptions. We have proposed to incorporate it into the traditional Information Retrieval (IR) architecture in building a Reverse Dictionary. To gain insights for the same, we have reported an experimental analysis of IR system based Wordster Reverse Dictionary.},
  author   = {Bushra Siddique and M M Sufyan Beg},
  doi      = {10.1007/s42979-022-01495-1},
  issn     = {2661-8907},
  issue    = {2},
  journal  = {SN Computer Science},
  pages    = {168},
  title    = {Reverse Dictionary Formation: State of the Art and Future Directions},
  volume   = {4},
  url      = {https://doi.org/10.1007/s42979-022-01495-1},
  year     = {2023}
}
@article{Vaswani2017,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  author   = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  month    = {6},
  title    = {Attention Is All You Need},
  url      = {http://arxiv.org/abs/1706.03762},
  year     = {2017}
}
