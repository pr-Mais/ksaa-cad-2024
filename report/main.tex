\documentclass[12pt]{article}
\linespread{1.5}
\setlength{\parindent}{0pt}

\addtolength{\footnotesep}{1.5mm}
\renewcommand{\thefigure}{\arabic{section}.\arabic{figure}}
\renewcommand{\thetable}{\arabic{section}.\arabic{table}}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[bottom]{footmisc}
\usepackage[hyperfootnotes=false]{hyperref}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{ragged2e}
\usepackage{multicol}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage{arabtex}
\usepackage{utf8}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[justification=raggedright,
            singlelinecheck=false,
            format=plain,
            labelfont={bf,it},
            textfont=it]{caption}
\usepackage[
    backend=biber,
    style=ieee,
    sorting=ynt
    ]{biblatex}
    
\addbibresource{export.bib}
\setcode{utf8}
\title{College of Computer and Information Sciences}
\author{Mais Alharaki}

\titleformat*{\section}{\Large\bfseries}
\titleformat*{\subsection}{\large\bfseries}
\titleformat*{\subsubsection}{\large\bfseries}
\titleformat*{\paragraph}{\large\bfseries}
\titleformat*{\subparagraph}{\large\bfseries}
\titlespacing*{\section}{0pt}{0pt}{0pt}
\titlespacing*{\subsection}{0pt}{0pt}{0pt}
\titlespacing*{\subsubsection}{0pt}{0pt}{0pt}

\newcolumntype{L}[1]{>{\hsize=#1\hsize\RaggedRight} X}

% Turn on the style
\pagestyle{fancy}
% Clear the header and footer
\fancyfoot{}
% Set the right side of the footer to be the page number
\fancyhf{}
\fancyhead[R]{Page \thepage\hspace{1pt} of~\pageref{lastpage}}
\pagestyle{fancy}

% Adjust section title spacing
% \titlespacing*{\section}{0pt}{0.2\baselineskip}{0.3\baselineskip}
% \titlespacing*{\subsection}{0pt}{0\baselineskip}{0\baselineskip}
% \titlespacing*{\subsubsection}{0pt}{0.1\baselineskip}{0.1\baselineskip}

\geometry{a4paper, margin=1in}
\setlength{\parskip}{1em}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\begin{document}

\begin{titlepage}
    
    \begin{figure*}[ht!]
        \centering
        \includegraphics[height=2cm]{2023.jpg}
        \hspace{4cm}
        \includegraphics[height=2cm]{pnu.jpg}
    \end{figure*}
    \vspace{.5cm}
    \centering
    {\textcolor{gray}{College of Computer and Information Sciences} \par}

    \vfill

    \centering
    {\LARGE\bfseries Research Project Report\par}
    \vspace{.5cm}
    {\large Master of Science in Computing (Data Science)\par}
    \vfill
    \begin{singlespace}
    \setstretch{1.2}
    \LARGE
    A Deep Learning Approach to Arabic Reverse Dictionary
    \end{singlespace}
    \vspace{2cm}

    \vfill
    
    \begin{center}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Student Name} & {Mais Alharaki} \\
        \hline
        \textbf{Student ID} & 444010562 \\ 
        \hline
        \textbf{Submission Date} & 2 May 2024 \\ 
        \hline
    \end{tabular}
    \end{center}

    \vfill
    
    Second Semester 2023--2024
\end{titlepage}

\begin{spacing}{-10}
    \normalsize
    \addtocontents{toc}{\vspace{3ex}}
    \tableofcontents{}
\end{spacing}

\newpage

\section*{Declaration}
\textbf{I hereby certify that:}
\begin{enumerate}
\item[-] This material, which I now submit for assessment on the programme of study leading to the award of MSc Computing (Data Science) is entirely my own work, and that I have exercised reasonable care to ensure that the work is original, and does not to the best of my knowledge breach any law of copyright, and has not been taken from the work of others save and to the extent that such work has been cited and acknowledged within the text of my work.
\item[-] Due acknowledgement has been given in the bibliography and references to ALL sources be they printed, electronic or personal.
\item[-] Unless this dissertation has been confirmed as confidential, I agree to an entire electronic copy or sections of the dissertation to be available to allow future students the opportunity to see examples of past dissertations.
\item[-] I agree to my dissertation being submitted to a plagiarism detection service, where it will be stored in a database and compared against work submitted from this or any other Department or from other institutions using the service. In the event of the service detecting a high degree of similarity between content within the service this will be reported back to my supervisor and program leader, who may decide to undertake further investigation that may ultimately lead to disciplinary actions, should instances of plagiarism be detected.
\item[-] I have read the \href{https://www.pnu.edu.sa/ar/Deanship/PostGraduate/Documents/دليل%20الأخلاقيات%20البحثية%20والأمانة%20العلمية%20والملكية%20الفكرية.pdf}{\underline{PNU Policy Statement on Ethics in Research}} and I confirm that ethical issues have been considered, evaluated and appropriately addressed in this research.
\end{enumerate}

\noindent
Signed:\\
Candidate Name: \underline{Mais Alheraki}\\
ID Number: \underline{444010562}\\
Date: \underline{2 May 2024}



\newpage

\thispagestyle{empty}
\listoffigures
\newpage
\thispagestyle{empty}
\listoftables
\newpage

\section*{List of abbreviations}
\begin{enumerate}
    \item[] RD: Reverse Dictionary
    \item[] CAD: Contemporary Arabic Dictionary
    \item[] SOTA: State of the Art
    \item[] NLP: Natural Language Processing
    \item[] NLG: Natural Language Generation
    \item[] NLU: Natural Language Understanding
    \item[] BERT: Bidirectional Encoder Representations from Transformers
    \item[] Seq2Seq: Sequence to Sequence
    \item[] LR: Learning Rate
\end{enumerate}
\newpage

\section*{Abstract}

The domain of reverse dictionaries, while advancing in languages like English and Chinese, remains underdeveloped for less Arabic. This study attempts to explore a data-driven approach to enhance word retrieval processes in Arabic RDs. The research focuses on the ArabicNLP 2024 Shared Task, named KSAA-CAD, which provides a dictionary dataset of 39,214 word-gloss pairs, each with a corresponding target word embedding. The proposed solution aims to surpass the baseline performance by employing state-of-the-art deep learning models and innovative data augmentation techniques. The methodology involves enriching the dataset with contextually relevant examples, training a T5 model to align the words to their glosses in the space, and evaluating the results using Mean Squared Error and Cosine similarity. We find that our model is closely aligned with the baseline performance on bertseg and bertmsa targets, however does not perform well on electra target, suggesting the need for further exploration of data augmentation techniques and model architectures to enhance Arabic RDs. The findings of this study contribute to the ongoing research in Arabic NLP and reverse dictionaries, providing insights into the challenges and opportunities in this domain.

\newpage

\section{Introduction}

While reverse dictionaries have witnessed advancements in languages like English and Chinese (e.g. WantWords \cite{Qi2020}), they remain less developed and explored for Arabic. This gap is particularly concerning for a language with a rich linguistic heritage and widespread use. The only ongoing effort in this domain is the 1st Arabic RD shared task launched in 2023 by King Salman Global Academy for Arabic Language (KSAA)\footnote{\url{https://arai.ksaa.gov.sa/sharedTask/}}.


\subsection{Content of study}

This research aims to explore new methods for Arabic RD, by proposing a data-driven approach that leverages deep learning to enhance word retrieval processes. We focus on the KSAA-CAD Shared Task dataset, which provides a rich collection of approximately 39k word-gloss pairs extracted from various Arabic dictionaries, each associated with a corresponding target word embedding. The proposed solution explores the performance of state-of-the-art deep learning models combined with data augmentation techniques.

\subsection{Problem statement \& motivation}

The Arabic language, with its intricate morphology and diverse dialects, presents unique challenges for Natural Language Processing (NLP) tasks. RDs are crucial tools for language learners, translators, and researchers, enabling them to identify words based on their meanings or descriptions.

However, the research and available tools within the Arabic RD domain are limited, with few studies exploring new methodologies or leveraging advanced deep learning models, which motivates the need for more research in this area.

The main research questions addressed in this study are:
\begin{enumerate}
    \item What is the impact of enriching the definitions from dictionary data with examples and expanding the dataset on performance?
    \item Are there any pre-trained architectures other than BERT that have similar good performance on Arabic?
\end{enumerate}

\subsection{Aim and objectives}

This project aims to enhance word retrieval processes in Arabic by leveraging data-driven techniques to establish a more intuitive and accurate connection between conceptual descriptions and corresponding dictionary words. This approach is specifically designed to improve the language learning experience by utilizing sophisticated data analytics to map natural language inputs to lexical outputs. Focusing on Arabic, a less explored language in this domain, presents a unique opportunity to contribute significantly to the field of data science by extending its applications to new linguistic territories.

To achieve this aim, the following objectives are set:
\begin{enumerate}
    \item Explore the KSAA-CAD dataset and understand the task requirements.
    \item Enrich the dataset with contextually relevant examples to enhance the model's understanding of word meanings.
    \item Fine-tune a pre-trained T5 model on the original and enriched dataset.
    \item Evaluate the model's performance using Mean Squared Error and Cosine similarity metrics.
    \item Compare the results with the baseline performance and discuss the findings.
    \item Submit a system design paper to the ArabicNLP 2024 conference based on the results.
\end{enumerate}

\subsection{Proposed solution}

We leverage the new KSAA-CAD dataset, which is shared exculsively with the participants of the 2024 RD Shared Task (more in section \ref{sec:arabicnlp}), comprising 39,214 word-gloss pairs with corresponding target word embeddings, alongside a SOTA baseline results\footnote{\url{https://github.com/ksaa-nlp/KSAA-CAD\#baseline-results}}. Our goal is to surpass the baseline performance by employing cutting-edge deep learning models and innovative data augmentation techniques. Figure \ref{fig:rd-flow} illustrates the Arabic RD system workflow. Our methodology is twofold:

\begin{figure}
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.9\textwidth]{rd-flow.png}
    \caption{The workflow of the arabic RD system}
    \label{fig:rd-flow}
\end{figure}

\begin{enumerate}
    \item \textbf{Data Enrichment:} The dataset will be enriched with contextually relevant examples to enhance the model's understanding of word meanings. This will be achieved by leveraging a large Arabic text corpus to curate examples for each word-gloss pair.
    \item \textbf{Pre-trained T5 Model Adaption:} A pre-trained T5 model \cite{Linting2021} will be fine-tuned on both the original and enriched KSAA-CAD dataset. We explore T5 for it being more recent and less explored in the Arabic RD literature.
\end{enumerate}

\subsection{Structure of the report}

The upcoming sections are structured as follows:

\begin{enumerate}
    \item Section 2 provides background material on natural language processing, deep learning, reverse dictionaries, and more.
    \item Section 3 reviews related work in the field of RDs, highlighting recent advancements and methodologies.
    \item Section 4 outlines our methodology, detailing the dataset, data enrichment techniques, and model architecture.
    \item Section 5 presents evaluation metrics, results of the experiments and discusses the findings.
\end{enumerate}

\newpage

\section{Background material}

This chapter lays the groundwork for understanding the subsequent sections of this report by exploring the fundamentals of natural language processing, deep learning, text embeddings, transformers, and more. These disciplines are instrumental to the overall research presented here.

\subsection{Natural Language Processing}

Language is the way we communicate and exchange information, it’s composed of symbols, rules and repetitive expressions. Natural Language Processing or NLP is a field where AI and linguistics cross together. It's interested in enabling machines to understand and model language, therefore enabling more natural communication between humans and machines.

NLP encompasses two major subfields: Natural Language Understanding (NLU) and Natural Language Generation (NLG). NLU focuses on enabling computers to comprehend the meaning of words, phrases, and expressions within human language. In contrast, NLG concerns the process of generating meaningful phrases and paragraphs, essentially allowing machines to "write" human-like text \cite{Khurana2023}.

\subsection{Deep learning}

Deep learning is a subfield of machine learning inspired by the structure and function of the human brain. It utilizes artificial neural networks (ANNs) with multiple hidden layers, enabling the extraction of complex patterns from large amounts of data. This capability has revolutionized various fields, including computer vision, natural language processing (NLP), and speech recognition.

Though the idea can be traced back many years, deep learning only recently has proved to outperform traditional ML algorithms in many areas, including NLP. In 2016, \cite{Vaswani2017} introduced the Transformer architecture, which has since become the foundation for many state-of-the-art NLP models, such as BERT, GPT-3, and T5.

However, training deep learning models from scratch often requires significant computational resources and large amounts of labeled data. This can be a barrier for tasks with limited data availability. Here's where transfer learning comes in as a powerful technique.

\subsubsection{Transfer learning}

Transfer learning is an approach that leverages knowledge gained from a pre-trained deep learning model on a source task and applies it to a new, related target task. By transferring the learned weights and features from a pre-trained model, a target model can achieve good performance even with limited training data. This is particularly beneficial for NLP tasks, where obtaining large amounts of labeled data can be expensive and time-consuming.

A popular example of transfer learning models borrowed from the computer vision field is the ImageNet \cite{ILSVRC15} dataset, where models trained on it can be used for other image classification tasks, naming VGG \cite{Simonyan2014}, ResNet \cite{He2015}, and Inception \cite{Simonyan2014}.

Models trained on large corupses of text are known as "language models", these models learn to assign probabilities to tokens (words or characters) given their occurance in the training corpus. Pre-trained language models can be used as a starting point, or as sometimes referred to as \textbf{checkpoints}, to a lot of downstream NLP tasks such as sentiment analysis, text classification, and topic modeling, instead of starting from scratch, which saves a vast amount of time and computational resources, given that a model have already learned the patterns from the original data.

Neural networks allowed for the development of more complex language models, where models can analyzed huge amounts of text data and learn the patterns and relationships between words, using various architectures such as RNNs, LSTMs, and Transformers.

\subsubsection{Sequence to Sequence approach for language modeling}

The sequence to sequence (Seq2Seq) approach is a powerful deep learning architecture specifically designed for tasks that involve processing and generating sequences. In NLP, this translates to tasks like machine translation, where an input sequence in one language is transformed into an output sequence in another language. Seq2Seq models typically consist of two deep neural networks:
\begin{enumerate}
    \item[-] Encoder: This network processes the input sequence and encodes it into a fixed-length vector representation capturing the semantic meaning.
    \item[-] Decoder: This network utilizes the encoded representation from the encoder and generates the output sequence one element at a time, conditioned on the previously generated elements.
\end{enumerate}

\subsubsection{Tokenization and text embeddings}

Tokenization is the process of breaking down text data into smaller units suitable for processing by deep learning models. These units can be words, characters, or even sub-word units like morphemes, which stands for a logical unit of a language that cannot be further divided (e.g. use, able, forming \textit{usable}). The choice of tokenization strategy can significantly impact the performance of deep learning models in NLP tasks.

Text embeddings represent the tokens as numerical vectors in a high-dimensional space, where words with similar meanings tend to be positioned closer together. Popular techniques for text embedding include Word2Vec and GloVe, which learn these representations by analyzing large text corpora, meaning a large set of texts (usually in electronic format) which is considered to be representative of a language.

In the case of deep learning models, text embeddings are often learned as part of the training process, and is often the result of the encoder part of a language model, where the input text is transformed into a fixed-length vector representation.

% Talk about Glove and Word2Vec

\subsubsection{SentencePiece tokenizer}

SentencePiece is an unsupervised text tokenizer, used by the T5 models for text data processing \cite{Kudo2018}. Unlike traditional word-based tokenization, SentencePiece utilizes subword units, which are smaller linguistic components like prefixes, suffixes, and morphemes. This approach offers several advantages. Firstly, it allows T5 to effectively handle OOV words by combining subword units to represent them. Secondly, SentencePiece is language-independent, enabling T5 to process text in diverse languages with a single tokenizer.

During tokenization, SentencePiece analyzes the training corpus to identify frequently occurring subword units and builds a vocabulary. When processing text, it segments the input into these subword units and assigns them unique token IDs, which the T5 model uses for its internal operations. This subword-level representation provides T5 with a more granular understanding of the text, enhancing its ability to perform various NLP tasks effectively.

\subsubsection{Transformer architecture: a paradigm shift}

The Transformer architecture, introduced by Vaswani et al \cite{Vaswani2017} in 2017 in the famous paper “Attention is All You Need”, has become a dominant force in NLP due to its ability to efficiently capture long-range dependencies within sequences. Unlike RNNs, which process sequences sequentially, Transformers rely solely on attention mechanisms. These mechanisms allow each element in the input sequence to attend to (focus on) other elements, enabling the model to understand the context of each word in relation to the entire sequence. This parallel processing approach facilitates faster training compared to RNNs and is particularly effective for tasks requiring long-range dependency modeling, such as machine translation and text summarization.

\subsubsection{Pre-trained transformer models: BERT and T5}

While the core Transformer architecture provides a powerful foundation, further advancements have led to the development of specialized pre-trained models like BERT and T5. These models leverage the strengths of Transformers and are pre-trained on massive amounts of unlabeled text data, allowing them to learn general contextual representations of language.

\begin{enumerate}
    \item[-] \textbf{BERT (Bidirectional Encoder Representations from Transformers)}: Introduced by Google AI \cite{devlin2018} in 2018, BERT is a pre-trained Transformer model that excels at understanding the context of words in a sentence and their relationships. It can be fine-tuned for various NLP tasks like question answering and sentiment analysis. However, BERT requires fine-tuning for specific tasks, which can be computationally expensive.
    \item[-] \textbf{T5 (Text-to-Text Transfer Transformer)}: Introduced also by Google AI in 2019 \cite{Raffel2019}, T5 utilizes a text-to-text format for all NLP tasks. It employs a single encoder-decoder architecture and learns to transform the input sequence into the desired output sequence. This approach makes T5 versatile, allowing it to handle a wide range of tasks by simply changing the format of the input and desired output. T5 often requires less fine-tuning compared to BERT, making it quicker to deploy for various tasks. However, it might not achieve the same level of deep contextual understanding as BERT in tasks where this is crucial.
\end{enumerate}

\subsection{Reverse dictionaries}

Dictionaries in their conventional form map words to a set of meanings or definitions, often combining them with some examples on how the words are used in context. Dictionaries are the foundation of various NLP tasks, serving as lexical resources, where they help in understanding information such as word meanings, parts of speech, and relationships between words. Tasks such as stemming and lemmatization also rely on dictionaries to return words to their base form. However, traditional dictionaries are unidirectional, providing word meanings based on the input word.

Reverse dictionaries are a form of dictionaries where a description yields a set of words from the dictionary that semantically matches the description. Traditionally used as tools for linguistic exploration, reverse dictionaries have evolved to play a significant role in data science. A prime use-case is their application in data exploration and analysis, where reverse dictionaries facilitate the identification of relevant features within complex textual datasets by generating key terms or phrases. This enhances the efficiency of data mining and fosters the discovery of new insights \cite{Chen2022}.

In machine learning, reverse dictionaries aid in feature engineering, enriching model inputs with nuanced context. This utility extends to automated metadata generation for effective data cataloging and management. For instance, they map specific terms (e.g., "machine learning") to broader categories (e.g., "computer science"). By analyzing the corpus for terms frequently appearing alongside these categories in the reverse dictionary, we can identify relevant metadata tags. This approach surpasses simple word frequency, utilizing relationships between terms for a richer description of the corpus content.

Additionally, they enhance content curation and recommendation systems, offering more precise content descriptors and improving recommendation relevance.

Furthermore, reverse dictionaries streamline text summarization and topic modeling \cite{Griffiths2004}, assisting in distilling essential information from large text volumes. They also play a crucial role in improving chatbot and customer service automation by accurately interpreting user queries and intents.

\subsection{Embedding-based retrieval}

Embedding-based retrieval is a technique that leverages text embeddings to retrieve relevant information from a dataset. By representing text as numerical vectors in a high-dimensional space, embedding-based retrieval models can efficiently search for semantically similar words or phrases. This approach is particularly useful for reverse dictionaries, where the goal is to look for the closest words to a query based on their descriptions or meanings (semantic relationship to the query) \cite{Huang2020}.

\subsection{ArabicNLP 2024 Shared Task}\label{sec:arabicnlp}

The KSAA-CAD Shared Task\footnote{Task has been launched in May 2024, more here: \href{https://arai.ksaa.gov.sa/sharedTask2024/}{arai.ksaa.gov.sa/sharedTask}} is a part of the ArabicNLP\footnote{\href{https://arabicnlp2024.sigarab.org/}{arabicnlp2024.sigarab.org}} conference for 2024, it aims towards developing an RD system capable of predicting words from their definitions in Arabic.

Participants are provided with a dataset containing 39k instances, and consisting of word-gloss pairs and corresponding word embeddings.

We particpated in this task, and the proposed solution is based on the dataset provided by the organizers. The aim is to outperform the baseline, and enhance the dataset with a new feature. Our solution will be evaluated through the shared task, and a corresponding system design paper (based on this report) will be submitted to the conference.

\newpage

\section{Related work}

Understanding the meaning behind words, even within the same language, presents a significant challenge for machines. Monolingual reverse dictionaries address this directly, aiming to identify a target word based on its definition in the same language. This task is particularly crucial for languages like Arabic, with its rich vocabulary and unique cultural nuances. However, research in this area remains less explored compared to other languages with rich resources.

\subsection{Previous studies}

One notable recent contribution is the work presented by ElBakry et al \cite{Albakry2023}. (2023) as part of the ArabicNLP 2023 Shared Task, where they demonstrate an approach to Arabic RD tasks, successfully handling both Arabic and English definition inputs. It utilizes an ensemble of fine-tuned BERT models, specifically CamelBERT-MSA and MARBERTv2, to predict word embeddings from provided definitions. By leveraging an ensemble strategy, the authors achieved improved results compared to single models, highlighting the benefits of this approach.

On the same task another attempt by Qaddoumi \cite{Qaddoumi2023}, a method is introduced to enhance Arabic word embeddings using a modified BERT Multilingual model with data augmentation, targeting improvements in Arabic RD tasks. By customizing BERT for Arabic and employing data augmentation strategies, the study achieves significant enhancements in semantic accuracy. However, it suggests further exploration into the effects of data augmentation and the need for expanded datasets.

Building on this, Sibaee et al. \cite{Sibaee2023} presently employs a SemiDecoder architecture combined with an SBERT encoder. This methodology excels in encoding word definitions into vectors using SBERT, followed by training with the SemiDecoder model. The approach leverages SBERT's proficiency in capturing semantic similarity and the SemiDecoder's training efficiency, leading to a high ranking in the shared task.

Other languages received more research in the area of RDs. Mane et al. \cite{Mane2022} proposed a unique approach to reverse dictionaries with mT5, aiming at Indian languages support, where mT5 was employed for its ability to understand and generate language across multiple languages. It contrasts with BERT's Masked Language Modeling, focusing instead on translating and understanding user inputs to produce accurate word predictions. The results showed that mT5 outperformed BERT-based models in the RD task for both Indian languages and an English baseline.

Ardoiz et al. \cite{Ardoiz2022}, in the SemEval RD task, studied the significance of high-quality lexicographic data in the efficiency of reverse dictionaries models. They suggest that refining the dataset by incorporating high-quality lexicographic data could significantly impact the task's outcomes, emphasizing the need for a robust dataset for optimal model performance. Their methodology involved a sentence-transformer model named “distiluse-base-multilingual-cased-v2”, which was trained to make the definition embeddings as similar as possible as the word gloss.

On the other hand, Tran et al. 2022 \cite{Tran2022} in SemEval RD task, evaluates Transformer-based models enhanced with LST and BiLSTM layers for RD across five languages, named English, Italian, Spanish, French and Russian, showcasing partial improvements over the CODWOE (COmparing Dictionaries and WOrd Embeddings) competition's baseline. It explores monolingual, multilingual, and zero-shot cross-lingual settings, providing insights into the viability of cross-lingual methodologies.

Chen et al. 2022 \cite{Chen2022} took a different approach on the English language by embedding both the definitions and words into the same shared space using transformer-based architectures to optimize the model across both tasks simultaneously. The model demonstrated superior performance in RD tasks, achieving high accuracy and consistency over previous methods. For definition modeling, while showing improvements, the results suggest areas for future enhancement, particularly in generating higher-quality definitions as indicated by human evaluations and BLEU scores.

Covering a specific instance of the English RD, Siddique et al. 2022 \cite{Siddique2022} focused on adjective phrases in Precisiated Natural Language, as mentioned that adjectives count for a large amount of content in natural language, hence highlighting the importance of a better representation for it. The proposed transformer-based model was reported to outperform the Onelook.com and WantWords online reverse dictionaries.

Following similar approaches in literature, Yan et al. 2020 \cite{Yan2020} incorporated BERT and mBERT into the RD task for both monolingual and cross-lingual contexts. The authors propose a method that enables effective word prediction from descriptions without needing parallel corpora for cross-lingual tasks. This approach addresses challenges such as data sparsity, polysemy, and the alignment of cross-lingual word embeddings. The methodology involved modifying the input sequence to include masked tokens that BERT or mBERT would predict, converting these predictions into word scores, and using these scores to rank the possible target words.

Zhang et al. 2019 \cite{Zhang2019} presents a cross-lingual, multi-channel RD model, addressing the variability of input queries and targeting both high and low-frequency words, showing state-of-the-art performance across English and Chinese datasets. The model combines a sentence encoder with multiple characteristic predictors (POS, morpheme, word category, sememe) to enhance word retrieval from descriptions. Experiments demonstrate significant improvements over conventional methods and commercial systems, particularly for human-written descriptions, while suggesting the model's adaptability to diverse linguistic features and robustness in handling variable inputs.

Finally, covering monolingual English RD, Pilehvar et al. 2019 \cite{Pilehvar2019} and Hedderich et al. 2019 \cite{Hedderich2019} emphasized on the importance of representing multi-sense words using different embeddings. Both methodologies address the limitations of single-sense embeddings by allowing for distinct representations of a word's different meanings, demonstrating substantial improvements in performance on the English language.

\begin{sidewaystable}
    \centering
    \caption{Summary of recent research on Reverse Dictionaries}
    \small
    \renewcommand{\arraystretch}{1.75}
    \begin{tabularx}{\textwidth}{|l|c|L{0.7}|L{1.1}|L{1}|L{1.2}|}
        \hline
        \textbf{Author} & \textbf{Year} &  \textbf{Language} & \textbf{Dataset} & \textbf{Methodology} & \textbf{Results} \\
        \hline
        Elbakry et al. \cite{Albakry2023} & 2023 & Arabic, English & KSAA ST 2023 RD & Ensemble of BERT models & 1st rank in the 2023 ST \\
        \hline
        Qaddoumi \cite{Qaddoumi2023} & 2023 & Arabic & KSAA ST 2023 RD & Modified mBERT model with data augmentation & 2nd rank in the 2023 ST \\
        \hline
        Sibaee et al. \cite{Sibaee2023} & 2023 & Arabic & KSAA ST 2023 RD & SemiDecoder architecture with SBERT encoder & 3rd rank in the 2023 ST \\
        \hline
        Mane et al. \cite{Mane2022} & 2022 & Indian & Hindi \& Marathi WordNet, English dictionary by \cite{Hill2016} & T5/mT5 based models & T5/mT5 outperformed BERT-based models \\
        \hline
        Ardoiz et al. \cite{Ardoiz2022} & 2022 & English & Data from SemEval 2022 Task\footnote{\href{https://github.com/TimotheeMickus/codwoe/blob/main/data/README.md}{https://github.com/TimotheeMickus/codwoe/blob/main/data/README.md}} & Sentence-transformer model & Importance of high-quality lexicographic data \\
        \hline
        Tran et al.\cite{Tran2022} & 2022 & English, Italian, Spanish, French, Russian & SemEval RD task 2023 & Transformer-based models with LST and BiLSTM layers & Partial improvements over baseline \\
        \hline
        Chen et al. \cite{Chen2022} & 2022 & English & Reverse dictionary & Transformer-based architectures for joint optimization & Superior performance in reverse dictionary tasks \\
        \hline
        Siddique \cite{Siddique2022} & 2022 & English & Reverse dictionary & Transformer-based model for adjective phrases & Outperforms online reverse dictionaries \\
        \hline
        Yan et al. \cite{Yan2020} & 2020 & English & Reverse dictionary & BERT and mBERT for monolingual and cross-lingual tasks & Effective word prediction from descriptions \\
        \hline
        Zhang et al. \cite{Zhang2019} & 2019 & English, Chinese & Reverse dictionary & Cross-lingual, multi-channel model with sentence encoder & State-of-the-art performance across English and Chinese datasets \\
        \hline
        Pilehvar et al. \cite{Pilehvar2019} & 2019 & English & Reverse dictionary & Multi-sense embeddings for multi-sense words & Improved performance on English RD \\
        \hline
    \end{tabularx}
\end{sidewaystable}

\subsection{Research gap}

Reading into the topic from literature proposed some questions:
\begin{enumerate}
    \item What is the impact of enriching the definitions from dictionary data with examples and expanding the dataset on performance?
    \item Are there any pre-trained architectures other than BERT that have similar good performance on Arabic?
\end{enumerate}

Moreover, Arabic RDs are not well discovered and researched for the Arabic language, which is evident by the fact that only few articles have explored it very recently in the literature. Our contribution in this domain will explore and attempt to answer both questions presented earlier.

At the end of this report, we will present the results of our experiments and discuss the findings, to determine wether the research question have been fullfilled, and if not, what are the limitations and future work that can be done to improve the results.

\newpage

\section{Methodology and proposed solution}

Most recent studies on reverse dictionaries have utilized pretrained models, as seen in the literature. Moreover, all studies on Arabic reverse dictionaries used BERT and its variations. Consequently, the potential for exploring other architectures and pre-trained models for the Arabic language remains intact. 

In this section, we go more in depth into the proposed solution, starting with understanding the dataset and task at hand, the methods used to enrich the dataset with relevant context, and finally the model architecture and evaluation results.

Briefly:
\begin{enumerate}
    \item The KSAA-CAD dataset contains 39k instances with its splits ready for experimentation.
    \item The first part of the pipeline concerns generating contextually close examples for each word:gloss pair in the KSAA-CAD dataset. In other words, creating a new feature that puts the lexical word into an example use. The goal is to enrich the context of each word's meaning in the dictionary.
    \item The inputs are then tokenized and encoded using SentencePiece tokenizer to prepare for training.
    \item A pre-trained T5 model \cite{Linting2021} is trained to predict the word embeddings. The architecture leverages sequence to sequence language modeling, an architecture that hasn’t been explored for Arabic RD in the literature.
    \item Finally, results are evaluated using Mean Squared Error and Cosine similarity, and compared with the baseline.
\end{enumerate}

% TODO create a figure showing the whole methodology pipeline

\subsection{Data description}

The KSAA-CAD dataset is an Arabic dictionary dataset containing \textbf{39,214 entries}, cleaned and ready for research, collected from various Arabic dictionaries. It isn't available publicly, and have been obtained as a result of participating in the ArabicNLP Shared Task for Reverse Dictionaries, as explained previously in section \ref{sec:arabicnlp}. We quote the dataset description from the Shared Task organizers:

\begin{quote}
    The first of these is the "Contemporary Arabic Language Dictionary" by Ahmed Mokhtar Omar (Omar, 2008), a resource previously utilized in the first iteration KSAA-RD. The second is the newly released dictionary of the Arabic contemporary language "Mu'jam Arriyadh" (Altamimi et al., 2023). The third is the "Al Wassit LMF Arabic Dictionary" (Namly, 2015). These dictionaries comprise words, commonly referred to as lemmas, and these may come with glosses, part of speech (POS), and examples.
\end{quote}

\begin{table}[H]
    \centering
    \caption{Dataset splits as provided by the Shared Task organizers}
    \label{table:dataset-splits}
    \renewcommand{\arraystretch}{1.5}%
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Train} & \textbf{Validation} & \textbf{Test} \\
        \hline
        31,372 & 3,921 & 3,921 \\
        \hline
    \end{tabularx}
\end{table}

The dataset is already split into train, validation and test sets, as seen in Table \ref{table:dataset-splits}, with 3 features named: word, gloss, pos, and 3 targets named: electra, bertseg, bertmsa. Table \ref{table:dataset-desc} demonstrates a sample entry from the dataset.

\begin{enumerate}
    \item A \textbf{word} is an entry in the dictionary, or a lemma.
    \item A \textbf{gloss} is the definition or meaning for this word based on its part of speech.
    \item A \textbf{pos} stands for Part-of-Speech, which is a grammatical tag assigned to words in NLP, and might include one of the following: noun, verb, adjective.
    \item The final 3 values represent our targets, each of them corresponding to a representation of the word in an independent multi-dimensional space using a set of different pretrained models, employing AraELECTRA \cite{Antoun2020-araELECTRA}, AraBERTv2 \cite{Antoun2020-araBERT}, and camelBERT-MSA \cite{inoue2021}, respectively referred to as \textbf{electra}, \textbf{bertseg}, and \textbf{bertmsa}.
\end{enumerate}

In Table \ref{table:dataset-desc}, the word \<عين> is a noun which means “The organ of vision in a living organism”, its part-of-speech is “Noun”, and is represented with 3 different word embeddings.

\begin{table}
    \centering
    \caption{Dataset description}
    \label{table:dataset-desc}
    \renewcommand{\arraystretch}{1.5}%
    \begin{tabularx}{\textwidth}{|l|X|}
        \hline
        \textbf{Feature} & \textbf{Value} \\
        \hline
        word & \<عين> \\
        \hline
        gloss & \<عضو الإبصار في الكائن الحي> \\
        \hline
        pos & n \\
        \hline
        electra & [0.4, 0.3, …] \\
        \hline
        bertseg & [0.7, 2.9, …] \\
        \hline
        bertmsa & [0.8, 1.4, …] \\
        \hline
    \end{tabularx}
\end{table}

\subsection{Features engineering: enriching the dataset}

As noticed while performing data exploration, the glosses are usually short and formal descriptions written by expert linguists. Table \ref{table:word-gloss} shows 2 words short and concise glosses, making its usage unclear, and might result in a vague understanding of the word.

Average users are unlikely to provide such precise descriptions, on the contrary, user queries might lack any key words that could identify the target word or set of words. Humans exhibit remarkable facility in acquiring new vocabulary from context early in childhood \cite{killian1995}. Therefore, and inspired by that capability, we propose that in order to enhance the model’s ability to align words and descriptions from user queries, we need to provide the model with more contextually relevant examples for each word-gloss pair.

The manual curation of contextually relevant examples is a laborious and resource-demanding task, which, given the short time frame of this experiment, is not a feasible solution, hence the need to find an automatic way to curate examples from publicly available Arabic datasets.

\begin{table}
    \centering
    \caption{Words with short glosses}
    \label{table:word-short-gloss}
    \renewcommand{\arraystretch}{1.5}%
    \begin{tabularx}{\textwidth}{|X|X|}
        \hline
        \textbf{Word} & \textbf{Gloss} \\
        \hline
        \<رقع> & \<ضربه بها> \\
        \hline
        \<قعد> & \<حبسه عنه> \\
        \hline
    \end{tabularx}
\end{table}

We chose the Arabic wikipedia embeddings dataset from the Embedding Archives project by CohereAI, which contains 3.1 million entries from Wikipedia, each entry containing a text, and the embedding of that text, alongside other metadata. Text embeddings in the dataset are achieved through CohereAI's \textit{multilingual-22-12}\footnote{\href{https://cohere.com/blog/multilingual}{https://cohere.com/blog/multilingual}} semantic embeddings model, trained for multilingual comprehension encompassing 101 languages including Arabic. This closed-source model is accessible via Cohere's API \cite{Kamalloo2023}.

To curate a number of new examples for each word in our dataset, we use a semantic similarity approach by embedding the word and gloss using \textit{multilingual-22-12} model, which is the same model that was used to embed Wikipedia text, then perform a vector similarity search using cosine similarity, to look for the top 5 closest entries from Wikipedia to the given word:gloss pairs. Figure \ref{fig:examples-generation} illustrates the process of generating examples for a \textit{word:gloss} pair.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.6\textwidth]{examples-generation.png}
    \caption{The process of generating a single example for a word:gloss pair}
    \label{fig:examples-generation}
\end{figure}

Table \ref{table:word-gloss} demonstrates an instance of the dataset with the new \textbf{example} feature, and as can be noticed, the feature may not directly contain the dictionary lemma \<كذاب>, but the surrounding context establishes a clear semantic relationship with the word's meaning.

\begin{table}[H]
    \centering
    \caption{A word with its gloss and newly added \textbf{examples} feature}
    \label{table:word-gloss}
    \renewcommand{\arraystretch}{1.5}%
    \begin{tabularx}{\textwidth}{|c|>{\centering\arraybackslash}c|>{\centering\arraybackslash}X|}
        \hline
        \textbf{Word} & \textbf{Gloss} & \textbf{Examples} \\
        \hline
        \RL{كذاب} & \RL{صيغة مبالغة من كذَبَ على: كثير الكذب} & {\RL{وردت لفظ الكذب ومشتقاتها في القرآن الكريم في مواضع متعددة وبصيغ متعددة.، ووردت بعدد (251) موضعًا، على (6) أوجه}\newline\RL{وهو أسوء أنواع الجهل، وهو الاِعْتِقَادُ الجَازِمُ بِمَا لاَ يَتَّفِقُ مَعَ الحَقِيقَةِ، إِذْ يَعْتَقِدُ الْمَرْءُ عَاِرفاً عِلْماً وَهُوَ عَكْسُ ذَلِكَ. وهو تعبيرٌ أُطلِقَ على من لا يسلِّم بجهله، ويدَّعى ما لا يعلم}} \\
        \hline
    \end{tabularx}
\end{table}

\subsubsection{Data pre-processing}

For machines to understand textual data, it must be preprocessed to represent it numerically. Fortunately, the dataset at hand is mostly clean and ready to be used, requiring only a \textbf{tokenization} step before training.

Data pre-processing pipeline can be summarized into two main steps:

\begin{enumerate}
    \item \textbf{Combining the gloss with the examples:} The dataset is augmented by merging the gloss and its corresponding top two examples, into a single text string that represents the input to the model. This step enriches the training data and provides the model with additional context.
    \item \textbf{Tokenization:} The input is then tokenized using the SentencePiece tokenizer, a pre-trained subword-level tokenization algorithm. This tokenizer is specifically trained for the mT5 model and is capable of handling multiple languages, including Arabic.
\end{enumerate}

Figure \ref{fig:tokenized-sentence} visualizes the result of tokenizing an Arabic sentence from the dataset, using our pre-trained tokenizer.

\begin{figure}
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\textwidth]{tokenized-sentence.png}
    \caption{A sample sentence tokenized using the mT5 tokenizer}
    \label{fig:tokenized-sentence}
\end{figure}

\subsection{Modeling}

This section goes through the modeling pipeline and the proposed architecture for the Arabic RD. The primary objective is to surpass the performance of the 2024 RD Shared Task baseline on the cosine similarity metric. 

We plan to achieve this objective by utilizing a retrieval framework that leverages 3 different word embeddings and cosine similarity measures for efficient retrieval of words based on its glosses. The end goal is to achieve a high alignment between the glosses and the words.

The methodolgy leverages transfer learning, where a pre-trained language model model is fine-tuned on the dataset to closely align glosses to words in the same multi-dimensional space. The words are already represented using the electra, bertseg, and bertmsa embeddings, which are considered as 3 independent targets in 3 different spaces.

The model architecture is based on the mT5 model \cite{Linting2021}, a variant of the T5 model \cite{Raffel2019}, which is a text-to-text transformer model. The mT5 model is pre-trained on a large corpus of multilingual text data, making it suitable for handling diverse languages, including Arabic.

\subsubsection{Model architecture}

The proposed model architecture leverages a pre-trained mT5 model as its foundation, particularly the encoder part of the model. The last hidden layer of the mT5 encoder contains the embedding information needed to represent the input sentence in the target words’ space. Figure \ref{fig:model-arch} illustrates the model architecture, which consists of an encoder, a pooling layer, and a linear layer.

The input is a vector of size 256, which is the tokenized and encoded input sentence. The input is then passed through the mT5 encoder, which outputs a matrix of shape (batch size, sequence length, hidden size). The last hidden state of the encoder is then passed through a pooling layer to transform it into a vector of a fixed length of 718. Finally, the output of the pooling layer is passed through a linear layer that transforms it into the desired target shape, which is 256 for electra, 768 for bertseg, and 768 for bertmsa.

\begin{figure}
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.8\textwidth]{model-arch.png}
    \caption{A sample sentence tokenized using the mT5 tokenizer}
    \label{fig:model-arch}
\end{figure}

The last hidden state of the mT5 encoder is not a vector, rather a matrix, thus we cannot use it directly as an input to the linear layer. To solve the problem, we introduced a pooling layer that transforms the hidden state matrix to a vector of a fixed length. This layer can be represented by equation \ref{eq:pooling}. This equation calculates a weighted sum of the last hidden states \(O\) where the weights are determined by the attention mask \(A\), normalized by the total weight (or count of non-zero entries in \(A\) for each sequence).

\begin{equation} \label{eq:pooling}
    pool = \frac{\sum_{j,k} (O_{ijk} \cdot A_{ijk})}{\sum_j A_{ij}}
\end{equation}

The final layer is a linear layer that takes the output from the pooling layer, and transforms it into the desired target shape. The dataset includes three targets with different shapes: 256 for electra, 768 for bertseg, and 768 for bertmsa. To accommodate these distinct shapes, the final layer of the model is adapted with two variations based on the target size, resulting in three different models.

The task at hand is not a direct sequence to sequence problem, rather an information retrieval problem. Therefore, the model's objective is to predict target words embeddings instead of the words themselves.

Finally, the pre-trained models employed for the encoder part are \textbf{mT5 Base}\footnote{\href{https://huggingface.co/google/mt5-base}{https://huggingface.co/google/mt5-base}} and \textbf{AraT5 V2}\footnote{\href{https://huggingface.co/UBC-NLP/AraT5-base}{https://huggingface.co/UBC-NLP/AraT5-base}}, both of which are variations of the T5 model. AraT5 V2 is a fine-tuned version of the mT5 base model on diverse Arabic data, making it more specialized for Arabic and suitable for our task \cite{Nagoudi2021}.

\newpage

\section{Data analysis and results}

\subsection{Environment setup}

This subsection details the software environment and hardware specifications used for feature engineering, model development, and experimentation. Primarily, the technical stack leverages Python and Google Colab for development.

For feature engineering, we utilized Postgres engine for SQL with the pgvector extension. This extension enables efficient storage of vector type data, and allow fast and efficient querying based on cosine similarity and other similarity measures. Additionally, the HNSW (Hierarchical Navigable Small World) algorithm is used for indexing, facilitating fast and accurate nearest neighbor searches within the high-dimensional vector space.

Model development was conducted within a Google Colab environment equipped with a single A100 GPU and 12 GB RAM. The chosen framework for this task was PyTorch, which is specifically designed for deep learning. Additionally, few other libraries were employed, notably the Hugging Face Transformers for loading pre-trained models with transformer architecture.

\subsection{Feature engneering analysis}

To generate a new feature representing a contextual example for a single entry in the dataset, we first merge the word and its gloss. As seen in Figure \ref{fig:examples-generation}, the word is \<بَسْط> and what comes after it is the gloss, which forms the input sentence. Next, the input is transformed into an embedding using the \textit{multilingual-22-12} model API, which results in a vector of size 768, this ensures we’re projecting the input into the Wikipedia dataset space. The next step is to find the closest texts to our input in the shared space, which is achievable using cosine similarity.

The process is repeated for each word:gloss pair in the dataset, generating a new feature named \textbf{examples}. The new feature is a list of strings, each string represents a contextually relevant example for the word:gloss pair. Table \ref{table:word-gloss} showcased an instance of the dataset with a newly added \textbf{examples} feature.

While enriching the glosses with additional context demonstrates promise, it is not without limitations. Random sampling of the results reveals instances where the retrieved entries exhibit a weaker semantic connection to the dictionary entry. Consequently, human intervention remains necessary for data labeling and refinement to ensure better quality, potentially augmented by incorporating additional data sources to enrich the retrieval process.

\subsection{Training}

% 

We used the training set which contains around 31k entries, to train a number of models. The Mean Square Error loss function is used to calculate the reconstruction loss, and Adam optimizer with a learning rate value of 3e-5 to optimize the weigths in the backward step, the LR has been obtained via trial and error. 

The archeticture and foundation model is T5 \cite{Linting2021}, however there's various checkpoints fine-tuned from the pre-trained original mT5 model. Among these checkpoints, we chose \textbf{mT5-base} (pre-trained and published by Google and original authors) and \textbf{AraT5v2} \cite{Nagoudi2021} (fine-tuned from mT5-base on a large Arabic corpus). Subsequent experiments were conducted on these two models, with the aim of comparing their performance on the task.

Briefly, the following experiments were conducted:
\begin{enumerate}
    \item \textbf{Glosses only as input}:
    \begin{enumerate}
        \item \textbf{mT5-base} fine-tuned to predict electra embeddings on the original training set.
        \item \textbf{mT5-base} fine-tuned to predict bertseg embeddings on the original training set.
        \item \textbf{mT5-base} fine-tuned to predict bertmsa embeddings on the original training set.
        \item \textbf{AraT5v2} fine-tuned to predict electra embeddings with target size 256.
        \item \textbf{AraT5v2} fine-tuned to predict bertseg embeddings with target size 768.
        \item \textbf{AraT5v2} fine-tuned to predict bertmsa embeddings with target size 768.
    \end{enumerate}
    \item \textbf{Glosses merged with 2 examples as input}:
    \begin{enumerate}
        \item \textbf{mT5-base} fine-tuned to predict electra embeddings on the augmented training set.
        \item \textbf{mT5-base} fine-tuned to predict bertseg embeddings on the augmented training set.
        \item \textbf{mT5-base} fine-tuned to predict bertmsa embeddings on the augmented training set.
        \item \textbf{AraT5v2} fine-tuned to predict electra embeddings with target size 256.
        \item \textbf{AraT5v2} fine-tuned to predict bertseg embeddings with target size 768.
        \item \textbf{AraT5v2} fine-tuned to predict bertmsa embeddings with target size 768.
    \end{enumerate}
\end{enumerate}

We will refer to the first set of experiments as \textbf{first experiment}, and the second set as \textbf{second experiment}.

The training setup is the same for all experiments, with the only difference being the target size, the input data and the number of epochs. Figure \ref{fig:training} shows the full training pipeline with target variations.

Based on the archeticture seen in Figure \ref{fig:model-arch}, the only trainable layer in our model is the linear layer, which is responsible for transforming the output of the pooling layer into the desired target shape. The rest of the model is frozen.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\textwidth]{training.png}
    \caption{The training pipeline with target variations}
    \label{fig:training}
\end{figure}

\newpage

\subsection{Evaluation results}

This subsection presents the final evaluation results of our experiments on the validation set. As the shared task withholds target values for the test set, its performance remains unevaluated. Once scores are submitted, official results will be announced alongside the winning solutions (those who outperform the baseline). Nonetheless, the validation set performance provides valuable insights into the model's potential.

Cosine similarity \ref{eq:cosine-similarity} and MSE \ref{eq:mse} are employed as evaluation metrics, measuring the semantic similarity between embeddings and the reconstruction error, respectively. Table \ref{table:results} summarizes the final results of our experiments compared to the baseline models. Moreover, Figure \ref{fig:results-chart} visualizes the results for better comparison.
% Cosine Similarity Equation
\begin{equation}\label{eq:cosine-similarity}
    Cosine Similarity = \frac{A \cdot B}{\|A\| \times \|B\|}
\end{equation}
% MSE Equation
\begin{equation}\label{eq:mse}
    MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2
\end{equation}
For cosine similarity, higher values (closer to 1) indicate better performance in capturing semantic relationships between the generated embeddings and the original word embedding. Lower MSE values indicate better reconstruction accuracy, meaning the model can more faithfully reproduce the original word embedding from the generated embeddings. The results presented are averaged across the validation set.

\begin{table}[H]
    \centering
    \caption{Results of the experiments on the validation set compared to the baseline. Numbers marked in bold indicate the best performance on the given target among the 3 targets.}
    \label{table:results}
    \renewcommand{\arraystretch}{1.5}%
    \begin{tabularx}{\textwidth}{|>{\raggedright}l|>{\raggedright\arraybackslash}X|X|X|}
        \hline
        \textbf{Model} & \textbf{Target} & \textbf{Cosine Similarity} & \textbf{MSE} \\
        \hline
        mT5-base (Ours) & electra & 0.5107 & 0.2498 \\\cline{2-4}
        with examples & bertseg & 0.7657 & 0.0806 \\\cline{2-4}
        & bertmsa & 0.7012 & 0.3434 \\\cline{2-4}
        \hline
        mT5-base (Ours) & electra & 0.5614 & 0.2274 \\\cline{2-4}
        & bertseg & 0.7763 & 0.0770 \\\cline{2-4}
        % TODO update the results
        & bertmsa & 0.7012 & 0.3434 \\\cline{2-4}
        \hline
        AraT5v2-base (Ours) & electra & 0.5152 & 0.2459 \\\cline{2-4}
        with examples & bertseg & 0.7656 & 0.0789 \\\cline{2-4}
        & bertmsa & 0.6965 & 0.3484 \\\cline{2-4}
        \hline
        AraT5v2-base (Ours) & electra & 0.5686 & 0.2255 \\\cline{2-4}
        & bertseg & 0.7752 & 0.0776 \\\cline{2-4}
        & bertmsa & 0.7140 & 0.3330 \\\cline{2-4}
        \hline
        CamelBERT (Baseline) & electra & \textbf{0.8185*} & 0.2195 \\\cline{2-4}
        & bertseg & \textbf{0.8436*} & 0.0555 \\\cline{2-4}
        & bertmsa & 0.5113 & 0.2428 \\\cline{2-4}
        \hline
        MARBERT (Baseline) & electra & 0.6948 & 0.5016 \\\cline{2-4}
        & bertseg & 0.7603 & 0.0818 \\\cline{2-4}
        & bertmsa & \textbf{0.7368*} & 0.1457 \\\cline{2-4}
        \hline
    \end{tabularx}
\end{table}

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\textwidth]{results-chart.png}
    \caption{Comparison of model performance on the validation set, \textbf{ours v2} stands for the models trained without examples.}
    \label{fig:results-chart}
\end{figure}

Overall, our models did not surpass the baseline models across all embedding types. Nonetheless, both mT5-base and AraT5v2-base models achieved competitive performance, particularly in predicting bertseg and bertmsa embeddings. The results indicate that the models are capable of capturing semantic relationships between words and glosses, as evidenced by the cosine similarity scores.

Notably, the results of the second experiment demonstrated worse performance compared to the first experiment. This discrepancy may be attributed to the quality of the augmented examples, which may not always provide meaningful context for the word:gloss pairs. Further refinement of the augmentation process and data quality may lead to improved performance. Another possible explanation is the targets provided by the shared task organizers, which was obtained from the definitions only, causing the model to overfit on the augmented examples.

\begin{table}[H]
    \centering
    \caption{Predictions for a sample word:gloss pair}
    \label{table:results-sample}
    \renewcommand{\arraystretch}{1.5}%
    \begin{tabularx}{\textwidth}{|c|c|>{\centering\arraybackslash}X|}
        \hline
        \textbf{Input} & \textbf{Top k=1 prediction} & \textbf{Ground truth} \\
        \hline
        \<منسوب إلى تاريخ، له أهمية كبيرة> & \<زراعي> & \<تاريخي> \\
        \hline
        \<جَرَّعَهُ الماءَ: سَقَاه إياه> & \<شِرْب> & \<جَرَّع> \\
        \hline
    \end{tabularx}
\end{table}

Table \ref{table:results-sample} showcases the model's predictions for two sample word:gloss pairs, highlighting the top retrieved predictions and the ground truth. It can be noticed that the model's predictions are not always accurate, with the first example predicting \<زراعي> instead of the correct \<تاريخي>, and the second example predicting \<شِرْب> instead of the correct \<جَرَّع>, though it can be seen that the predictions have some syntactic and semantic similarity to the ground truth, indicating that the model is learning some meaningful relationships between the words.

Possible explanations for the performance gap include the model's architecture, training duration, and the quality of the augmented examples. We may require further fine-tuning or additional training epochs to achieve optimal performance. Moreover, the quality of the augmented examples may impact the model's ability to learn meaningful word embeddings, necessitating further refinement.

To further enhance the model's performance, we propose the following:
\begin{enumerate}
    \item \textbf{Hyperparameter tuning:} optimizing hyperparameters like learning rate, batch size, and optimizer settings could potentially improve performance.
    \item \textbf{Better resources for data enrichment:} refining the process of generating contextually relevant examples to ensure higher quality and more meaningful context for the word:gloss pairs.
    \item \textbf{Advanced architectures:} exploring more sophisticated architectures like larger pre-trained models (e.g., mT5-large) or incorporating techniques like selective training of specific model components could potentially lead to significant performance gains.
    \item \textbf{Human evaluation:} conducting a human evaluation of the model's predictions to assess the quality of the generated word embeddings and identify areas where the model excels or falls short.
\end{enumerate} 

\newpage

\section{Conclusion}

This report presents a comprehensive overview of the Arabic RD task, detailing the dataset, methodology, and results of our experiments. The proposed solution leverages a pre-trained mT5 model for word embedding retrieval, with the goal of surpassing the baseline performance on the cosine similarity metric. The model is trained on augmented examples generated from the Arabic Wikipedia dataset, aiming to enrich the glosses with additional context for improved performance.

While we did not outperform the baseline on all metrics, we achieved promising results. AraT5 generally performed slightly better than mT5, suggesting the effectiveness of fine-tuning on a specialized Arabic model.

While incorporating contextual examples did not imrove the results on this task, we belive this is regarded to several reasons including the model single-layer archeticture and shared task targets. The model's performance could be enhanced through hyperparameter tuning, exploring advanced architectures, and potentially using transfer learning or human evaluation for further refinement.

Our contributions include:
\begin{enumerate}
    \item Exploring the potential of mT5 archeticture for Arabic RD tasks, which have not been extensively studied in the literature.
    \item Introducing a novel approach to enriching glosses with contextually relevant examples, aiming to improve the model's performance and building an enhanced RD dataset.
    \item Conducting a comprehensive evaluation of the proposed solution, providing insights into the model's strengths and limitations.
\end{enumerate}

Future work will focus on: 
\begin{enumerate}
    \item Refining the data enrichment process, possibly integrating new resources other than Wikipedia.
    \item Exploring more advanced architectures and hyperparameter tuning to enhance the model's performance.
    \item Conducting human evaluations to assess the models performance and identify areas for improvement.
\end{enumerate}

Finally, we believe that the proposed solution has the potential to significantly enhance the performance of Arabic RD models, and we hope that our findings will inspire further research in this domain.

\newpage

\printbibliography[title={References}]\label{lastpage}


\end{document}



