{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "\n",
    "\n",
    "def read_json(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_json(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_csv(path: str, index: str = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, index_col=index).dropna(axis=1, how=\"all\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_dict = read_csv(\"../data/arabic-dictionary/riyadh_dict.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic feature engineering: adding a new feature to the dataset\n",
    "\n",
    "Each word in the dictionary has a definition. The definition is a sentence that describes the meaning of the word. However, we can notice that a word can have multiple meanings. For example, the word \"أبدا\" appeard 2 times, in the first time it means \"never\" and in the second time it means \"forever\". So, we will add a new feature `example` to the dataset to indicate the meaning of the word in a real sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wikipedia embeddings\n",
    "\n",
    "In the first method, we will use a dataset containing the word embeddings of the Arabic Wikipedia. The dataset is available at [this link](https://huggingface.co/datasets/Cohere/wikipedia-22-12-ar-embeddings). These embeddings are encoded using the [cohere.ai multilingual-22-12](https://txt.cohere.ai/multilingual/) embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import Index, Text, create_engine, select, text\n",
    "from sqlalchemy import Column, Integer\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "from pgvector.sqlalchemy import Vector\n",
    "\n",
    "engine = create_engine(os.environ.get(\"CONNECTION_STRING\"))\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class MixinSearch:\n",
    "    @classmethod\n",
    "    def fulltext_search(cls, session, search_string):\n",
    "        return (\n",
    "            session.query(cls)\n",
    "            .filter(\n",
    "                text(f\"search @@ websearch_to_tsquery('arabic', '{search_string}')\")\n",
    "            )\n",
    "            .limit(20)\n",
    "            .all()\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def vector_search(cls, session, vector):\n",
    "        return session.scalars(\n",
    "            select(cls).order_by(cls.emb.cosine_distance(vector)).limit(5)\n",
    "        ).all()\n",
    "\n",
    "\n",
    "class Wiki(MixinSearch, Base):\n",
    "    \"\"\"Wiki model.\"\"\"\n",
    "\n",
    "    __tablename__ = \"wiki_embeds\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    text = Column(Text)\n",
    "    emb = Column(Vector(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the wikipedia dataset for Arabic, no splits required.\n",
    "ar_wiki_dataset = load_dataset(\n",
    "    \"Cohere/wikipedia-22-12-ar-embeddings\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True,\n",
    "    download_mode=\"reuse_cache_if_exists\",\n",
    "    split=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "import cohere\n",
    "\n",
    "co = cohere.Client()\n",
    "\n",
    "\n",
    "def embed_text(texts: List[str]) -> List[float]:\n",
    "    response = co.embed(texts=texts, model=\"multilingual-22-12\")\n",
    "    query_embedding = response.embeddings\n",
    "\n",
    "    return query_embedding[0]\n",
    "\n",
    "\n",
    "def find_matching_sentences(word: str, table, window_size=100) -> List[str]:\n",
    "    matching_sentences: List[str] = []\n",
    "\n",
    "    if not word or not isinstance(word, str):\n",
    "        return matching_sentences\n",
    "\n",
    "    results = table.vector_search(session, embed_text(word))\n",
    "    print(results[0].text)\n",
    "\n",
    "    for text_obj in results:\n",
    "        sentence = text_obj.text\n",
    "        for match in re.finditer(r\"\\b\" + re.escape(word) + r\"\\b\", sentence):\n",
    "            if match:\n",
    "                span = match.span()\n",
    "                context_start = max(0, span[0] - window_size)\n",
    "                context_end = min(len(sentence), span[1] + window_size)\n",
    "                context = sentence[context_start:context_end]\n",
    "                matching_sentences.append(context)\n",
    "\n",
    "    return matching_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|██████████| 95207/95207 [1:08:20<00:00, 23.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fetch_and_process_row(index):\n",
    "    row = ar_dict.iloc[index]\n",
    "    query = f\"({row['UnDiacWord']}) {row['Definition']}\"\n",
    "    matching_sentences = Wiki.vector_search(session, embed_text([query]))\n",
    "    matching_sentences = [x.text for x in matching_sentences]\n",
    "    return {\n",
    "        **row.to_dict(),\n",
    "        \"examples\": matching_sentences,\n",
    "    }\n",
    "\n",
    "\n",
    "matching_sentences_dict = []\n",
    "dict_range = range(ar_dict.shape[0])\n",
    "\n",
    "# Initialize a ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Use a list comprehension to submit all tasks and collect Future objects\n",
    "    futures = [executor.submit(fetch_and_process_row, i) for i in dict_range]\n",
    "\n",
    "    # Use tqdm to iterate over futures as they complete\n",
    "    for future in tqdm(\n",
    "        concurrent.futures.as_completed(futures),\n",
    "        total=len(futures),\n",
    "        desc=\"Processing Rows\",\n",
    "    ):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            matching_sentences_dict.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Row processing generated an exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(matching_sentences_dict).to_csv(\"examples_cohere_embeds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_id': 1,\n",
       " 'Id': 9698,\n",
       " 'Word': 'تَأْبِيد',\n",
       " 'UnDiacWord': 'تأبيد',\n",
       " 'mainPOS': 'اسم',\n",
       " 'PoS': 'اسم معنى',\n",
       " 'Definition': 'تَأْبِيد الأمرِ: تخليده وإبقاؤه مدى الدهر.',\n",
       " 'examples': ['التَّودُّد لغةً: من الوُدِّ، والوُدُّ مصدر الموَدَّة. والوُدُّ هو الحُبُّ ويكون في جميع مداخل الخير، والتواد التحاب.',\n",
       "  '- نص - (إذا سرق الحاكم ثيران الشعب وأخفاها أو عاث بحقولهم وزرعهم أو أعطاها إلى الأجنبي فإن أدّاد سيكون له بالمرصاد، وإذا استولى على غنمهم فإن أدد ساقي الأرض والسماء سيبيد ماشيته في مراعيها وسيجعلها طعامًا للشمس).',\n",
       "  'قال أبو حاتم: (الواجب على العاقل أنْ يتحبَّب إلى النَّاس بلزوم حسن الخلق، وترك سوء الخلق؛ لأنَّ الخلق الحسن يذيب الخطايا كما تذيب الشَّمس الجليد، وإن الخلق السَّيئ ليفسد العمل كما يفسد الخلُّ العسل، وقد تكون في الرَّجل أخلاق كثيرة صالحة كلُّها، وخلق سيئ، فيفسد الخلق السَّيئ الأخلاق الصَّالحة كلَّها).',\n",
       "  'فأيَّده النَّبي صلى الله عليه وسلم على تودُّدِه إليهم، وإن لم يجد منهم مقابلًا لما يقوم به، إلَّا الإساءة إليه.',\n",
       "  'ولم يكن استعمال هذا المصطلح القرآني في التصوف إلا اتباعا لتعريف الجنيد البغدادي لطريق المتعبدين، حينما قال:']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_sentences_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import ast\n",
    "\n",
    "\n",
    "def generate_context(entries: List[dict]) -> str:\n",
    "    contexts = []\n",
    "    for entry in entries:\n",
    "        context = f\"\"\"\n",
    "        You are a linguist working on a project to enrich a dictionary of Arabic words with examples.\n",
    "\n",
    "        You've been given a list of words and their definitions, each word has a list of generated examples from a large corpus of Arabic text. Look for semantically close examples for each word based on its definition that helps the reader understand the meaning of the word. \n",
    "        Differentiate between the different POS and meanings of the word.\n",
    "\n",
    "        Your output must match the following format, do not change it:\n",
    "        word: example1, example2, ...\n",
    "\n",
    "        Do not put any other titles or explanations. Your output number of lines must match the number of words. Commit to the words given to you, don't go beyond or provide any other words.\n",
    "\n",
    "        Example:\n",
    "        إبرة: إبرة الحقن هي إبرة مجوفة تستخدم عادة مع المحقن لحقن مادة في الجسم\n",
    "\n",
    "        Input starts here:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        examples = ast.literal_eval(entry[\"examples\"])[:7]\n",
    "        example = examples[0] if examples else \"\"\n",
    "        context += f\"word: {entry['word']}\\ndefinition: {entry['definition']}\\nPOS: {entry['pos']}\\nMain POS: {entry['mainـpos']}\\nexamples: {examples}\\n\\n\"\n",
    "\n",
    "        contexts.append(context)\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to GPT-4 API\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "for m in genai.list_models():\n",
    "    if \"generateContent\" in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.0-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_dict = examples.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos : pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for group in chunker(examples_dict, 100):\n",
    "    examples_dict_modified = []\n",
    "\n",
    "    for entry in group:\n",
    "        prompt = generate_context([entry])[0]\n",
    "\n",
    "        try:\n",
    "            if eval(entry[\"examples\"]) != []:\n",
    "                response = model.generate_content(\n",
    "                    prompt, generation_config={\"temperature\": 0.1}\n",
    "                )\n",
    "                entry[\"gemini_example\"] = \" \".join(response.text.split(\":\")[1:])\n",
    "                print(f\"{entry['word']} :: {entry['gemini_example']}\")\n",
    "            else:\n",
    "                entry[\"gemini_example\"] = \"empty\"\n",
    "        except Exception as e:\n",
    "            response = model.generate_content(prompt)\n",
    "            print(\"rejected\")\n",
    "\n",
    "            entry[\"gemini_example\"] = \"rejected\"\n",
    "\n",
    "        examples_dict_modified.append(entry)\n",
    "\n",
    "        # Delay to avoid rate limiting\n",
    "        # time.sleep(.5)\n",
    "\n",
    "    pd.DataFrame(examples_dict_modified).to_json(\n",
    "        f\"examples_modified_{i}.json\", index=False, orient=\"records\", force_ascii=False\n",
    "    )\n",
    "\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
