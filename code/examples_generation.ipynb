{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook where examples were generated. Trying to run it will result in an error cause it requires a Postgres database with Wikipedia embeddings, a HNSW index on the embeddings, and a Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_json(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_json(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_csv(path: str, index: str = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, index_col=index).dropna(axis=1, how=\"all\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_dict = read_csv(\"../data/arabic-dictionary/riyadh_dict.csv\")\n",
    "train_df = pd.read_json(\"../data/shared-task/train.json\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic feature engineering: adding a new feature to the dataset\n",
    "\n",
    "Each word in the dictionary has a definition. The definition is a sentence that describes the meaning of the word. However, we can notice that a word can have multiple meanings. For example, the word \"أبدا\" appeard 2 times, in the first time it means \"never\" and in the second time it means \"forever\". So, we will add a new feature `example` to the dataset to indicate the meaning of the word in a real sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wikipedia embeddings\n",
    "\n",
    "In the first method, we will use a dataset containing the word embeddings of the Arabic Wikipedia. The dataset is available at [this link](https://huggingface.co/datasets/Cohere/wikipedia-22-12-ar-embeddings). These embeddings are encoded using the [cohere.ai multilingual-22-12](https://txt.cohere.ai/multilingual/) embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import Index, Text, create_engine, select, text\n",
    "from sqlalchemy import Column, Integer\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "from pgvector.sqlalchemy import Vector\n",
    "\n",
    "engine = create_engine(os.environ.get(\"CONNECTION_STRING\"))\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class MixinSearch:\n",
    "    @classmethod\n",
    "    def fulltext_search(cls, session, search_string):\n",
    "        return (\n",
    "            session.query(cls)\n",
    "            .filter(\n",
    "                text(f\"search @@ websearch_to_tsquery('arabic', '{search_string}')\")\n",
    "            )\n",
    "            .limit(20)\n",
    "            .all()\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def vector_search(cls, session, vector):\n",
    "        return session.scalars(\n",
    "            select(cls).order_by(cls.emb.cosine_distance(vector)).limit(5)\n",
    "        ).all()\n",
    "\n",
    "\n",
    "class Wiki(MixinSearch, Base):\n",
    "    \"\"\"Wiki model.\"\"\"\n",
    "\n",
    "    __tablename__ = \"wiki_embeds\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    text = Column(Text)\n",
    "    emb = Column(Vector(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c968dd019642f69949ba5ca5289b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the wikipedia dataset for Arabic, no splits required.\n",
    "ar_wiki_dataset = load_dataset(\n",
    "    \"Cohere/wikipedia-22-12-ar-embeddings\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True,\n",
    "    download_mode=\"reuse_cache_if_exists\",\n",
    "    split=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "import cohere\n",
    "\n",
    "co = cohere.Client()\n",
    "\n",
    "\n",
    "def embed_text(texts: List[str]) -> List[float]:\n",
    "    response = co.embed(texts=texts, model=\"multilingual-22-12\")\n",
    "    query_embedding = response.embeddings\n",
    "\n",
    "    return query_embedding[0]\n",
    "\n",
    "\n",
    "def find_matching_sentences(word: str, table, window_size=100) -> List[str]:\n",
    "    matching_sentences: List[str] = []\n",
    "\n",
    "    if not word or not isinstance(word, str):\n",
    "        return matching_sentences\n",
    "\n",
    "    results = table.vector_search(session, embed_text(word))\n",
    "    print(results[0].text)\n",
    "\n",
    "    for text_obj in results:\n",
    "        sentence = text_obj.text\n",
    "        for match in re.finditer(r\"\\b\" + re.escape(word) + r\"\\b\", sentence):\n",
    "            if match:\n",
    "                span = match.span()\n",
    "                context_start = max(0, span[0] - window_size)\n",
    "                context_end = min(len(sentence), span[1] + window_size)\n",
    "                context = sentence[context_start:context_end]\n",
    "                matching_sentences.append(context)\n",
    "\n",
    "    return matching_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 94/31372 [00:05<31:48, 16.39it/s] \n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fetch_and_process_row(index):\n",
    "    row = train_df.iloc[index]\n",
    "    query = f\"({row['word']}) {row['gloss']}\"\n",
    "    matching_sentences = Wiki.vector_search(session, embed_text([query]))\n",
    "    matching_sentences = [x.text for x in matching_sentences]\n",
    "    return {\n",
    "        **row.to_dict(),\n",
    "        \"examples\": matching_sentences,\n",
    "    }\n",
    "\n",
    "\n",
    "matching_sentences_dict = []\n",
    "dict_range = range(train_df.shape[0])\n",
    "\n",
    "# Initialize a ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Use a list comprehension to submit all tasks and collect Future objects\n",
    "    futures = [executor.submit(fetch_and_process_row, i) for i in dict_range]\n",
    "\n",
    "    # Use tqdm to iterate over futures as they complete\n",
    "    for future in tqdm(\n",
    "        concurrent.futures.as_completed(futures),\n",
    "        total=len(futures),\n",
    "        desc=\"Processing Rows\",\n",
    "    ):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            matching_sentences_dict.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Row processing generated an exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(matching_sentences_dict).to_json(\n",
    "    \"../data/shared-task/train_with_examples.json\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_id': 1,\n",
       " 'Id': 9698,\n",
       " 'Word': 'تَأْبِيد',\n",
       " 'UnDiacWord': 'تأبيد',\n",
       " 'mainPOS': 'اسم',\n",
       " 'PoS': 'اسم معنى',\n",
       " 'Definition': 'تَأْبِيد الأمرِ: تخليده وإبقاؤه مدى الدهر.',\n",
       " 'examples': ['التَّودُّد لغةً: من الوُدِّ، والوُدُّ مصدر الموَدَّة. والوُدُّ هو الحُبُّ ويكون في جميع مداخل الخير، والتواد التحاب.',\n",
       "  '- نص - (إذا سرق الحاكم ثيران الشعب وأخفاها أو عاث بحقولهم وزرعهم أو أعطاها إلى الأجنبي فإن أدّاد سيكون له بالمرصاد، وإذا استولى على غنمهم فإن أدد ساقي الأرض والسماء سيبيد ماشيته في مراعيها وسيجعلها طعامًا للشمس).',\n",
       "  'قال أبو حاتم: (الواجب على العاقل أنْ يتحبَّب إلى النَّاس بلزوم حسن الخلق، وترك سوء الخلق؛ لأنَّ الخلق الحسن يذيب الخطايا كما تذيب الشَّمس الجليد، وإن الخلق السَّيئ ليفسد العمل كما يفسد الخلُّ العسل، وقد تكون في الرَّجل أخلاق كثيرة صالحة كلُّها، وخلق سيئ، فيفسد الخلق السَّيئ الأخلاق الصَّالحة كلَّها).',\n",
       "  'فأيَّده النَّبي صلى الله عليه وسلم على تودُّدِه إليهم، وإن لم يجد منهم مقابلًا لما يقوم به، إلَّا الإساءة إليه.',\n",
       "  'ولم يكن استعمال هذا المصطلح القرآني في التصوف إلا اتباعا لتعريف الجنيد البغدادي لطريق المتعبدين، حينما قال:']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_sentences_dict[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
