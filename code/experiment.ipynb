{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "\n",
    "\n",
    "def read_json(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_json(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_csv(path: str, index: str = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, index_col=index).dropna(axis=1, how=\"all\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_dict = read_csv(\"../data/arabic-dictionary/riyadh_dict.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic feature engineering: adding a new feature to the dataset\n",
    "\n",
    "Each word in the dictionary has a definition. The definition is a sentence that describes the meaning of the word. However, we can notice that a word can have multiple meanings. For example, the word \"أبدا\" appeard 2 times, in the first time it means \"never\" and in the second time it means \"forever\". So, we will add a new feature `example` to the dataset to indicate the meaning of the word in a real sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dd0b4ee5f548c5a3a0607630a0f7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1219201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the wikipedia dataset for Arabic, no splits required.\n",
    "ar_wiki_dataset = load_dataset(\n",
    "    \"wikimedia/wikipedia\",\n",
    "    \"20231101.ar\",\n",
    "    trust_remote_code=True,\n",
    "    download_mode=\"reuse_cache_if_exists\",\n",
    ")\n",
    "\n",
    "ar_wiki_dataset_pd = ar_wiki_dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import DDL, Text, create_engine, func, text\n",
    "from sqlalchemy import Column, Integer\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import TSVECTOR\n",
    "\n",
    "CON_STRING = \"postgresql://mais@localhost:5432/postgres\"\n",
    "engine = create_engine(CON_STRING)\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class MixinSearch:\n",
    "\n",
    "    @classmethod\n",
    "    def fulltext_search(cls, session, search_string):\n",
    "        return session.query(cls)\\\n",
    "            .filter(text(f\"search @@ websearch_to_tsquery('arabic', '{search_string}')\")).limit(5).all()\n",
    "\n",
    "\n",
    "class Wiki(MixinSearch, Base):\n",
    "    \"\"\"Wiki model.\"\"\"\n",
    "\n",
    "    __tablename__ = \"wiki\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    text = Column(Text)\n",
    "\n",
    "    # Index should be created after the table is created.\n",
    "    # ALTER TABLE wiki ADD search tsvector GENERATED ALWAYS AS (to_tsvector('arabic', text)) STORED;\n",
    "    # CREATE INDEX search_idx ON wiki USING GIN (search);\n",
    "    search = Column(TSVECTOR)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219201, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_wiki_dataset_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:43<00:00,  7.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# Insert the data into the database\n",
    "from tqdm import tqdm\n",
    "\n",
    "chunks = [ar_wiki_dataset_pd[i:i + 100000] for i in range(0, ar_wiki_dataset_pd.shape[0], 100000)]\n",
    "\n",
    "for chunk in tqdm(chunks):\n",
    "    chunk.to_sql(\n",
    "        Wiki.__tablename__, con=engine, if_exists=\"append\", index=False, chunksize=10000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>UnDiacWord</th>\n",
       "      <th>mainPOS</th>\n",
       "      <th>PoS</th>\n",
       "      <th>Definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9698</td>\n",
       "      <td>تَأْبِيد</td>\n",
       "      <td>تأبيد</td>\n",
       "      <td>اسم</td>\n",
       "      <td>اسم معنى</td>\n",
       "      <td>تَأْبِيد الأمرِ: تخليده وإبقاؤه مدى الدهر.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9699</td>\n",
       "      <td>مُؤَبَّد</td>\n",
       "      <td>مؤبد</td>\n",
       "      <td>صفة</td>\n",
       "      <td>صفة مفعول</td>\n",
       "      <td>غير منهي، دائم ومستمر.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9700</td>\n",
       "      <td>أَبَد</td>\n",
       "      <td>أبد</td>\n",
       "      <td>اسم</td>\n",
       "      <td>اسم معنى</td>\n",
       "      <td>زمن طويل غير محدود.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9701</td>\n",
       "      <td>أَبَدًا</td>\n",
       "      <td>أبدا</td>\n",
       "      <td>حرف (أداة)</td>\n",
       "      <td>اسم مبهم</td>\n",
       "      <td>دومًا، طوال الزمن.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9702</td>\n",
       "      <td>أَبَدًا</td>\n",
       "      <td>أبدا</td>\n",
       "      <td>حرف (أداة)</td>\n",
       "      <td>اسم مبهم</td>\n",
       "      <td>مُطلقًا‏، تستخدم للنفي المطلق.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id    Id      Word UnDiacWord     mainPOS        PoS  \\\n",
       "0        1  9698  تَأْبِيد      تأبيد         اسم   اسم معنى   \n",
       "1        2  9699  مُؤَبَّد       مؤبد         صفة  صفة مفعول   \n",
       "2        3  9700     أَبَد        أبد         اسم   اسم معنى   \n",
       "3        4  9701   أَبَدًا       أبدا  حرف (أداة)   اسم مبهم   \n",
       "4        4  9702   أَبَدًا       أبدا  حرف (أداة)   اسم مبهم   \n",
       "\n",
       "                                   Definition  \n",
       "0  تَأْبِيد الأمرِ: تخليده وإبقاؤه مدى الدهر.  \n",
       "1                      غير منهي، دائم ومستمر.  \n",
       "2                         زمن طويل غير محدود.  \n",
       "3                          دومًا، طوال الزمن.  \n",
       "4              مُطلقًا‏، تستخدم للنفي المطلق.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 27299/95207 [02:58<05:41, 198.82it/s]"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "session.close()\n",
    "\n",
    "def find_matching_sentences(word: str, table, window_size=100) -> List[str]:\n",
    "    matching_sentences: List[str] = []\n",
    "\n",
    "    if not word or not isinstance(word, str):\n",
    "        return matching_sentences\n",
    "\n",
    "    results = table.fulltext_search(session, word)\n",
    "\n",
    "    for text_obj in results:\n",
    "        sentence = text_obj.text\n",
    "        for match in re.finditer(r\"\\b\" + re.escape(word) + r\"\\b\", sentence):\n",
    "            if match:\n",
    "                span = match.span()\n",
    "                context_start = max(0, span[0] - window_size)\n",
    "                context_end = min(len(sentence), span[1] + window_size)\n",
    "                context = sentence[context_start:context_end]\n",
    "                matching_sentences.append(context)\n",
    "\n",
    "    return matching_sentences\n",
    "\n",
    "\n",
    "matching_sentences_dict = []\n",
    "dict_range = range(ar_dict.shape[0])\n",
    "\n",
    "for i in tqdm(dict_range):\n",
    "    row = ar_dict.iloc[i]\n",
    "    word = row[\"UnDiacWord\"]\n",
    "    matching_sentences = find_matching_sentences(word, Wiki, 1000)\n",
    "    matching_sentences_dict.append(\n",
    "        {\n",
    "            \"id\": row[\"Id\"],\n",
    "            \"word_id\": row[\"word_id\"],\n",
    "            \"word\": word,\n",
    "            \"undiac_word\": row[\"UnDiacWord\"],\n",
    "            \"pos\": row[\"PoS\"],\n",
    "            \"mainـpos\": row[\"mainPOS\"],\n",
    "            \"definition\": row[\"Definition\"],\n",
    "            \"examples\": matching_sentences,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(matching_sentences_dict).to_csv(\"examples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    The following is a list of Arabic paragraphs from Wikipedia, \\n    each paragraph with an accompaying word and definition from the dictionary. \\n    Extract a sentence to use as an example for each word.\\n\\n    Your output must match the following format:\\n    word: sentence\\n\\n    Do not put any other titles or explanations. \\n    Your output number of lines must match the number of words.\\n    Commit to the words given below, don't go beyond or provide any other words.\\n\\n    Example:\\n    إبرة: إبرة الحقن هي إبرة مجوفة تستخدم عادة مع المحقن لحقن مادة في الجسم\\n\\n    Input starts here:\\n    word: إبرة\\ndefinition: إبرة الحقن هي إبرة مجوفة تستخدم عادة مع المحقن لحقن مادة في الجسم\\nparagraph: إبرة الحقن هي إبرة مجوفة تستخدم عادة مع المحقن لحقن مادة في الجسم\\n\\nword: ماء\\ndefinition: الماء هو سائل شفاف عديم اللون والطعم والرائحة\\nparagraph: الماء هو سائل شفاف عديم اللون والطعم والرائحة\\n\\nword: مدرسة\\ndefinition: المدرسة هي مؤسسة تعليمية تقوم بتعليم الطلاب\\nparagraph: المدرسة هي مؤسسة تعليمية تقوم بتعليم الطلاب\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def generate_context(entries: List[dict]) -> str:\n",
    "    context = f\"\"\"\n",
    "    The following is a list of Arabic paragraphs from Wikipedia, \n",
    "    each paragraph with an accompaying word and definition from the dictionary. \n",
    "    Extract a sentence to use as an example for each word.\n",
    "\n",
    "    Your output must match the following format:\n",
    "    word: sentence\n",
    "\n",
    "    Do not put any other titles or explanations. \n",
    "    Your output number of lines must match the number of words.\n",
    "    Commit to the words given below, don't go beyond or provide any other words.\n",
    "\n",
    "    Example:\n",
    "    إبرة: إبرة الحقن هي إبرة مجوفة تستخدم عادة مع المحقن لحقن مادة في الجسم\n",
    "\n",
    "    Input starts here:\n",
    "    \"\"\"\n",
    "\n",
    "    for entry in entries:\n",
    "        context += f\"word: {entry['word']}\\ndefinition: {entry['paragraph']}\\nparagraph: {entry['definition']}\\n\\n\"\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "result = generate_context(\n",
    "    [\n",
    "        {\n",
    "            \"word\": \"إبرة\",\n",
    "            \"definition\": \"إبرة الحقن هي إبرة مجوفة تستخدم عادة مع المحقن لحقن مادة في الجسم\",\n",
    "            \"examples\": [\"1\", \"2\"],\n",
    "        } \n",
    "    ]\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
